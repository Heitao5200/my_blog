# 引言-大模型基础

### 什么是语言模型

#### 评估句子的工具

语言模型就像一个智能系统，告诉你哪些单词排列在一起听起来更像是真正的、自然的句子

为所有可能的序列分配一个合理的概率

这种表达方式是合乎语法且有意义的。这个句子遵循了英语的主谓宾结构，同时符合我们的常识理解。

#### 生成全新的文本

抛骰子决定每个单词，只不过每个单词被选中的概率不同，完全取决于它在句子中的自然程度和上下文相关性。

#### 自回归语言模型

利用已知的单词来预测接下来的单词,通过一个称为概率的链式法则的数学公式来工作

模型试图理解，给定之前的单词，下一个单词是什么的可能性。在数学上，我们将文本序列抽象为$x_{1:L}$ 的联合分布 $$p_(x_{1:L} )$$ 表示这个自回归语言模型预测下一个词的概率，使用概率的链式法常见写法是：

自回归语言模型的特点是它可以利用例如前馈神经网络等方法有效计算出每个条件概率分布

当$$T$$值较高时，我们会获得更平均的概率分布，生成的结果更具随机性；反之，当 $$T$$值较低时，模型会更倾向于生成概率较高的词元。

### 大模型的相关历史

香农的论文《通信的数学理论》中奠定了信息理论的基础。并引入了信息熵的概念

熵实际上是一个衡量将样本$x～p$编码内容的随机性，与内容本身无关。不管是什么样内容的文件，只要服从同样的概率分布，就会计算得到同样的信息熵。熵的值越小，表明序列的结构性越强，需要编码的长度就越短

交叉熵H(p,q)是熵H(p)的上界



#### N-gram模型

语音识别和机器翻译系统使用了基于词的n-gram语言模型

如果n太小，那么模型将无法捕获长距离的依赖关系，下一个词将无法依赖于𝖲𝗍𝖺𝗇𝖿𝗈𝗋𝖽。然而，如果n太大，统计上将无法得到概率的好估计



#### 神经语言模型



使用链接打卡的小伙伴将获得助教的评审，同时获得评优资格。
============
链接导航栏
============
【课程链接】
我们的学习内容在：https://github.com/datawhalechina/so-large-lm/tree/main我们更新主要在wiki上面：https://github.com/datawhalechina/so-large-lm/wiki
【小程序使用手册】
https://mp.weixin.qq.com/s/iPmzb72Yk0mhIA2NYezXDg
【学习者手册】
https://mp.weixin.qq.com/s/wXSCQ9fcit7gL0fwAwbtCQ
【队长手册】
https://datawhale.feishu.cn/docs/doccn0YOPvNhGt54Tb2JjxLhDmc#PFdbLL