# 模型架构

## 大模型之模型概括

将语言模型的看作一个黑箱，其可以根据输入需求的语言描述（prompt）生成符合需求的结果（completion）
$$
prompt \overset{model}{\leadsto} completion \ \ or \ \ model(prompt) = completion
$$
大型语言模型的构建需要考虑**分词**和**模型架构**：

- **分词**（Tokenization）：即如何将一个字符串拆分成多个词元。
- **模型架构**（Model architecture）：我们将主要讨论Transformer架构，这是真正实现大型语言模型的建模创新。

## 分词

语言模型 $p$ 是建立在词元（token）序列的上的一个概率分布输出，其中每个词元来自某个词汇表$V$，

```text
[the, mouse, ate, the, cheese]
```

自然语言并不是以词元序列的形式出现，而是以字符串的形式存在。**分词器**将任意字符串转换为词元序列

### 基于空格的分词

对于英文字母来说，由于其天然的主要由单词+空格+标点符号组成，最简单的解决方案是使用`text.split(' ')`方式进行分词，这种分词方式对于英文这种按照空格，且每个分词后的单词有语义关系的文本是简单而直接的分词方式。然而，对于一些语言，如中文，句子中的单词之间没有空格。即使在英语中，也有连字符词（例如father-in-law）和缩略词（例如don't），它们需要被正确拆分。例如，**Penn Treebank将don't拆分为do和n't**，这是一个在语言上基于信息的选择，但不太明显。因此，仅仅通过空格来划分单词会带来很多问题。

那么，什么样的分词才是好的呢？目前从直觉和工程实践的角度来说：

- 首先我们**不希望有太多的词元**（极端情况：字符或字节），否则序列会变得难以建模。
- 其次我们也**不希望词元过少**，**否则单词之间就无法共享参数**（例如，mother-in-law和father-in-law应该完全不同吗？），这对于形态丰富的语言尤其是个问题（例如，阿拉伯语、土耳其语等）。
- **每个词元应该是一个在语言或统计上有意义的单位**。



### BPE(Byte pair encoding)

将**字节对编码（[BPE](https://zh.wikipedia.org/wiki/%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81)）算法**应用于数据压缩领域，用于生成其中一个最常用的分词器。BPE分词器需要通过模型训练数据进行学习，获得需要分词文本的一些频率特征。

学习分词器的过程，直觉上，我们先将每个字符作为自己的词元，并组合那些经常共同出现的词元。整个过程可以表示为：

- Input(输入)：训练语料库（字符序列）。
- 算法步骤
  - Step1. 初始化词汇表 $V$ 为字符的集合。
  - while(当我们仍然希望V继续增长时)：
    - Step2. 找到$V$中共同出现次数最多的元素对 $x,x'$ 。
  - Step3. 用一个新的符号 $xx'$ 替换所有 $x,x'$ 的出现。
  - Step4. 将$xx'$ 添加到V中。

#### Unicode的问题

Unicode编码对BPE分词产生了一个问题（尤其是在多语言环境中），在训练数据中我们不可能见到所有的字符。可以对字节运行BPE算法（[Wang等人，2019年](https://arxiv.org/pdf/1909.03341.pdf)）。
以中文为例：
$$
\text { 今天} \Rightarrow \text {[x62, x11, 4e, ca]}
$$

BPE算法的作用是**减少数据的稀疏性**。通过对字节级别进行分词，可以在多语言环境中更好地处理Unicode字符的多样性，并减少数据中出现的低频词汇，提高模型的泛化能力。通过使用字节编码，可以将不同语言中的词汇统一表示为字节序列，从而更好地处理多语言数据。



### Unigram model (SentencePiece)

Unigram model**定义一个目标函数来捕捉一个好的分词的特征**，

给定一个序列 $x_{1:L}$ ，一个分词器 $T$ 是 $p\left(x_{1: L}\right)=\prod_{(i, j) \in T} p\left(x_{i: j}\right)$ 的一个集合。这边给出一个实例：

- 训练数据（字符串）： $𝖺𝖻𝖺𝖻𝖼$
- 分词结果  $T={(1,2),(3,4),(5,5)}$ （其中 $V=\{𝖺𝖻,𝖼\}$ ）
- 似然值： $p(x_{1:L})=2/3⋅2/3⋅1/3=4/27$

unigram 模型通过统计每个词汇在训练数据中的出现次数来估计其概率。$p(𝖺𝖻)=2/3$ ，$p(𝖼)=1/3$ 。

较高的似然值表示训练数据与分词结果之间的匹配程度较高，这意味着该分词结果较为准确或合理。

#### 3.2.3.1 算法流程

- 从一个“相当大”的种子词汇表 $V$ 开始。
- 重复以下步骤：
  - 给定 $V$ ，使用EM算法优化 $p(x)$ 和 $T$ 。
  - 计算每个词汇 $x∈V$ 的 $loss(x)$ ，衡量如果将 $x$ 从 $V$ 中移除，似然值会减少多少。
  - 按照 $loss$ 进行排序，并保留 $V$ 中排名靠前的80%的词汇。

这个过程旨在优化词汇表，剔除对似然值贡献较小的词汇，以减少数据的稀疏性，并提高模型的效果。通过迭代优化和剪枝，词汇表会逐渐演化，保留那些对于似然值有较大贡献的词汇，提升模型的性能。

## 模型架构

### 语言模型分类

语言模型分为三个类型：**编码端（Encoder-Only）**，**解码端（Decoder-Only）**和**编码-解码端（Encoder-Decoder）** 

#### 编码端（Encoder-Only）架构

编码端架构的著名的模型如BERT、RoBERTa等。这些语言模型生成**上下文向量表征**，但**不能直接用于生成文本**。**通常用于分类任务**（也称为自然语言理解任务）。任务形式比较简单，下面以情感分类/自然语言推理任务举例：

$$
情感分析输入与输出形式：[[CLS], 他们, 移动, 而, 强大]\Rightarrow 正面情绪
$$

$$
自然语言处理输入与输出形式：[[CLS], 所有, 动物, 都, 喜欢, 吃, 饼干, 哦]⇒蕴涵
$$

优点：对于每个 $x{i}$ ，上下文向量表征可以双向地依赖于左侧上下文 $(x_{1:i−1})$ 和右侧上下文  $(x_{i+1:L})$ 。

缺点：不能自然地生成完成文本，且需要更多的特定训练目标（如掩码语言建模）。

#### 解码器（Decoder-Only）架构

这些是我们常见的自回归语言模型,GPT系列模型，给定一个提示  $x_{1:i}$ ，它们可以生成上下文向量表征，并对下一个词元 $x_{i+1}$ （以及递归地，整个完成  $x_{i+1:L}$） 生成一个概率分布。 $x_{1:i}⇒ϕ(x_{1:i}),p(x_{i+1}∣x_{1:i})$ 。我们以自动补全任务来说，

输入与输出的形式为
$$
[[CLS], 他们, 移动, 而]⇒强大
$$



优点：能够自然地生成完成文本，有简单的训练目标（最大似然）。

缺点：对于每个  $xi$ ，上下文向量表征只能单向地依赖于左侧上下文  ($x_{1:i−1}$) 。

####  编码-解码端（Encoder-Decoder）架构

编码-解码端架构就是最初的**Transformer模型**，其他的还有如**BERT、T5**等模型。这些模型在某种程度上结合了两者的优点：它们可以使用双向上下文向量表征来处理输入 $x_{1:L}$ ，并且可以生成输出 $y_{1:L}$ 。可以公式化为：

$$
x1:L⇒ϕ(x1:L),p(y1:L∣ϕ(x1:L))。
$$

以表格到文本生成任务为例，其输入和输出的可以表示为：

$$
[名称:, 植物, |, 类型:, 花卉, 商店]⇒[花卉, 是, 一, 个, 商店]。
$$



优点：每个 $x_{i}$ ，**上下文向量表征可以双向地依赖于左侧上下文**  $x_{1:i−1}$ ) 和右侧上下文 ( $x_{i+1:L}$ )，可以自由的生成文本数据。

缺点：**需要更多的特定训练目标**。

### 语言模型理论

#### 基础架构

首先将词元序列转换为序列的向量形式。 $EmbedToken$ 函数通过在嵌入矩阵 $E∈ℝ^{|v|×d}$ 中查找每个词元所对应的向量，该向量的具体值这是从数据中学习的参数：

然后定义一个抽象的 $SequenceModel$ 函数，它接受这些上下文无关的嵌入，并将它们映射为上下文相关的嵌入。



#### 递归神经网络

#### SequenceRNN

#### RNN

注：

- 简单RNN由于梯度消失的问题很难训练。
- 为了解决这个问题，发展了长短期记忆（LSTM）和门控循环单元（GRU）（都属于RNN）。
- 然而，即使嵌入h200可以依赖于任意远的过去（例如，x1），它不太可能以“精确”的方式依赖于它（更多讨论，请参见Khandelwal等人，2018）。
- 从某种意义上说，LSTM真正地将深度学习引入了NLP领域。

#### Transformer

- Decoder-Only（GPT-2，GPT-3）

- Encoder-Only（BERT，RoBERTa）

- Encoder-Decoder（BART，T5） 





##### 残差连接和归一化

**残差连接**：计算机视觉中的一个技巧是残差连接（ResNet）。我们不仅应用某个函数f：
$$
f(x1:L)，
$$
而是添加一个残差（跳跃）连接，以便如果$f$的梯度消失，梯度仍然可以通过 $x_{1:L}$ 进行计算：

$$
x_{1:L}+f(x_{1:L})。
$$

**层归一化**:它接收一个向量并确保其元素不会太大：

##### 位置嵌入

词元的嵌入不依赖于其在序列中的位置，因此两个句子中的𝗆𝗈𝗎𝗌𝖾将具有相同的嵌入，从而在句子位置的角度忽略了上下文的信息，这是不合理的。

```
[𝗍𝗁𝖾,𝗆𝗈𝗎𝗌𝖾,𝖺𝗍𝖾,𝗍𝗁𝖾,𝖼𝗁𝖾𝖾𝗌𝖾]
[𝗍𝗁𝖾,𝖼𝗁𝖾𝖾𝗌𝖾,𝖺𝗍𝖾,𝗍𝗁𝖾,𝗆𝗈𝗎𝗌𝖾]
```

为了解决这个问题，我们将位置信息添加到嵌入中：

def $EmbedTokenWithPosition(x_{1:L}:ℝ^{d×L})$ ：

- 添加位置信息。
- 定义位置嵌入：
  - 偶数维度：$P_{i,2j}=sin(i/10000^{2j/dmodel})$
  - 奇数维度：$P_{i,2j+1}=cos(i/10000^{2j/dmodel})$
- 返回 $[x_1+P_1,…,x_L+P_L]$ 。

上面的函数中， $i$ 表示句子中词元的位置， $j$ 表示该词元的向量表示维度位置。



