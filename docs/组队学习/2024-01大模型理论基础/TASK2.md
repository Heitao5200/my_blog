# 大模型的能力

## 概述

### 对比每个任务的最新技术成果，为什么GPT-3的效果参差不齐？

GPT-3并未明确针对这些任务进行训练，仅仅是被用来预测下一个词

并未过度拟合，意味着它有很大的潜力在许多其他任务上表现良好

如果希望GPT-3在特定的任务上表现很好，需要使用大量的标签数据进行微调

## 语言模型的适应性：从语言模型到任务模型的转化

在自然语言处理的世界中，语言模型 $p$是一种对token序列 $x_{1:L}$ 的分布

将语言模型转化为任务模型的过程称作适应（Adaptation）这个过程需要以下两个输入：

1. 任务的自然语言描述
2. 一组训练实例（输入-输出对）

适应的两种方式：

- **训练（标准的有监督学习）**：训练一个新模型，使其能将输入映射到输出。
  - 通过创建一个新模型并利用语言模型作为特征（探针法），
  - 从现有的语言模型出发，根据训练实例进行更新（微调），或者在这两者之间找到平衡（轻量级的微调）
- **提示（上下文）学习**：根据对任务的描述建一个或一组提示/上下文信息，将其输入到语言模型中以获取基于该任务的生成结果。根据提示/上下文信息的数量，我们还可以进一步细分：
  - 零样本学习(Zero-shot)：提示/上下文信息的数量为0，模型直接基于对任务的理解输出结果。
  - 单样本学习(One-shot)：提示/上下文信息的数量为1，一般来说模型基于1个例子可以更好的理解任务从而较好的生成结果。
  - 少样本学习(Few-shot)：提示/上下文信息的数量大于1，大模型可以看到更丰富的例子，一般来说获得比单样本学习更好的效果。
  - **局限性**：只能利用少量的训练实例（一般情况只能塞进一个提示的数量）。这种输入的局限性由于Transformer自身的局限性导致的，模型可输入的长度具有约束（一般来讲是2048个tokens）



对于一个任务需要考虑以下几点：

- 定义：任务是什么，以及其动机？
- 适应：我们如何通过提示将任务简化为语言模型？
- 结果：与该任务的最先进模型相比，GPT-3的定量性能如何？

因为模型的大小和训练样本的数量都很重要，默认设置：

- 完整的GPT-3模型（davinci），其拥有1750亿参数。
- 使用尽可能多的使用训练数据的实例进行上下文学习。

#### Language Modeling

对GPT-3的各种功能有深入的认知，并真正理解如何优化给模型的提示（当前只通过基于提出信息就可以获得性能的提示已经成为了共识）。最直观的方法是验证语言模型是否能够有效地模仿和理解语言。

语言模型 $p$是关于词汇序列的概率分布。假设我们有一段文本$x_{1:L}$,语言模型会给这段文本分配什么概率？

我们可以将联合概率分解为每个token的条件概率的乘积，通过链式规则完成的:
$$
p(x_{1:L}) = \prod_{i=1}^L p(x_i \mid x_{1:i-1}).
$$
##### 困惑度

语言模型性能使用**困惑度（Perplexity）**进行评估：
$$
P(X) = P(x_1,x_2,...,x_N)^{(-1/N)}
$$
**困惑度（Perplexity）**表示模型在预测下一个词时的平均不确定性，如果一个模型的困惑度较低，那么它在预测下一个词的时候就会更加准确。

**缺点**：一个序列的联合概率取决于其长度，并且随着长度的增长，其值趋近于零

优化：

一个序列的联合概率取决于其长度，并且随着长度的增长，其值趋近于零，因此困惑度会越来越小

对每一个词标记的概率进行平均就可以评估模型在处理各种词标记时的平均性能。

- 算术平均
  - 如果给一个词标记分配了0的概率，有一个非常低的概率（如0）可能会被其他较高的概率抵消。

- **几何平均**
  - 每个词标记的概率都被同等看待，并且一个极低的概率（如0）将会导致整个几何平均大幅度下降

**两类错误**：语言模型可能会犯两种类型的错误，而困惑度对这两种错误的处理方式并不对称：

- 召回错误：语言模型未能正确地为某个词符分配概率值。例如，如果模型为词组 '𝖺𝗍𝖾' 在 '𝗍𝗁𝖾,𝗆𝗈𝗎𝗌𝖾' 后出现的概率预测为接近0，那么对应的困惑度值将趋近于无穷大。

$$
p({ate} \mid {the}, {mouse}) \to 0 \quad\Rightarrow\quad \text{perplexity}_p({the}, {mouse}, {ate}, {the}, {cheese}) \to \infty.
$$

- 精确度错误：语言模型为某些错误的词序列过度分配了概率值。在这种情况下，困惑度会进行适度的惩罚。给定一个语言模型 p，假设我们将一些垃圾分布 $r$ 按照概率 $ϵ$ 混入：

$$
q(x_i \mid x_{1:i-1}) = (1-\epsilon) p(x_i \mid x_{1:i-1}) + \epsilon r(x_i \mid x_{1:i-1}).
$$





### Penn Tree Bank

[Penn Tree Bank](https://catalog.ldc.upenn.edu/LDC99T42) 是自然语言处理中的一个经典数据集，最初是为了进行句法解析而标注的。

####  [LAMBADA](https://arxiv.org/pdf/1606.06031.pdf)

预测句子的最后一个词

####  [HellaSwag](https://arxiv.org/pdf/1905.07830.pdf)

评估模型进行常识推理的能力，**多项选择**

给定一个问题 x，你如何对候选答案 y 进行评分呢？没有明确的答案，但这里有一些启发式方法：
- 未归一化的概率(Unnormalized probability)： $score(x,y)=p(x,y)$ 。未归一化概率的问题是它倾向于短答案。
- 长度归一化概率(Length-normalized probability)： $score(x,y)=p(x,y)/num-tokens(y)$ 。这修正了长度偏见。然而，对于长度相同的两个答案，模型仍可能偏好更受欢迎的实体。
- 频率归一化概率(Frequency-normalized probability)： $score(x,y)=p(y∣x)/p(y∣x_{0})$ ，其中 $x_{0}$ 是一个中立的字符串，如'Answer:'。这降低了恰巧很常见的答案（例如，“John”）的得分。

### Question answering



####  [TriviaQA](https://arxiv.org/pdf/1705.03551.pdf)

任务：给定一问题后生成答案
原始数据集是由业余爱好者收集的，并被用作开放式阅读理解的挑战，但我们用它来进行（闭卷）问题回答。我们根据训练实例和问题定义一个提示，并将完成的内容作为预测的答案：

#### [WebQuestions](https://aclanthology.org/D13-1160.pdf)

任务：和TriviaQA类似是问答任务
数据集从Google搜索查询中收集，最初用于对知识库的问题回答。我们定义一个提示，就如TriviaQA一样（演示）

#### NaturalQuestions

任务：回答问题
从Google搜索查询中收集的数据集（区别在于答案的长度较长）我们和上面一样定义一个提示：

### Translation

将源语言（例如，德语）中的句子翻译成目标语言（例如，英语）中的句子

### Arithmetic

做算术题（2-5位数的加法，减法，乘法）你没有实际的理由要解决这个问题；这只是一个诊断任务，满足我们的科学好奇心。我们将问题提出为问题回答：

### News article generation

任务：给定标题和副标题，生成新闻文章。
数据集：标题/副标题取自[newser.com](https://stanford-cs324.github.io/winter2022/lectures/capabilities/newser.com)。
我们设立了一个评估标准，人类根据文章可能由机器编写的可能性对文章进行评分。

### Novel tasks

#### 使用新词

任务：给定一个新造的词和定义，生成使用该词的句子。
我们依旧只需在提示中描述任务

#### 纠正英语语法

任务：给定一个不合语法的句子，生成其合语法的版本。
我们通过给出提示来描述任务（提示是有输入和输入对组成的）

### Other tasks

自原始论文以来，GPT-3已应用于许多更多的任务，包括基准数据集(Benchmark)和一次性的演示(one-off deoms)。以下是一个不详尽的列表:
**Benchmarks：**

- [SWORDS](https://arxiv.org/pdf/2106.04102.pdf)：词汇替换，目标是在句子的上下文中预测同义词。
- [Massive Multitask Language Understanding](https://arxiv.org/pdf/2009.03300.pdf)：包括数学，美国历史，计算机科学，法律等57个多选问题。
- [TruthfulQA](https://arxiv.org/pdf/2109.07958.pdf)：人类由于误解而错误回答的问答数据集。
  **结果：**虽说GPT-3在这些Benchmark数据集中的表现平庸，但是考虑到我们只使用了few-shot的情况，或许不算太差。

**one-off Demos：**

- [Examples from the OpenAI website](https://beta.openai.com/examples/)
- [Examples from gpt3demo.com](https://gpt3demo.com/)
  这些演示既创新又有趣，但很难判断它们的可靠性如何。



