# 大模型的数据

对于大型语言模型来说，训练数据就是“**原始文本(涵盖广泛的领域、类型、语言)**”。

## 大语言模型背后的数据



[**Common Crawl**](https://en.wikipedia.org/wiki/Common_Crawl)是一个非营利组织，它对网络进行爬取，并提供免费给公众的快照。由于其便利性，它已经成为许多模型如**T5、GPT-3和Gopher**的**标准数据源**。 Common Crawl在2021年4月的快照就有320TB的数据 

网络数据存在的问题：
- 大规模数据在全球人口中的**代表性仍然不均衡**。
- 网络数据过多地代表了来自**发达国家的年轻用户**。
- GPT-2的训练数据基于Reddit，根据皮尤互联网研究的2016年调查，美国Reddit用户中有67%是男性，**64%的年龄在18到29岁**之间。
- **维基百科**的**编者**中只有**8.8-15%是女性**。
- 网络上的骚扰可能会让某些人群（如跨性别者、神经发育不同的人）产生排斥感。
- 过滤"不良词汇"可能进一步边缘化某些人群（如LGBT+）。
因此，我们的结论是：理解和记录用于训练大型语言模型的数据集的组成是至关重要的。

### WebText和OpenWebText数据集

**WebText数据集被用于训练GPT-2模型**

#### 创建数据集的过程

- 创建WebText数据集(40GB)的过程包括：

  - 抓取至少获得3个赞的所有外链，


  - 过滤掉维基百科以便在基于维基百科的基准测试中进行评估，

  - OpenAI并没有公开发布WebText数据集，


- 创建[OpenWebText数据集](https://skylion007.github.io/OpenWebTextCorpus/)(38GB)的过程包括：

  - 从[Reddit提交的数据集](https://files.pushshift.io/reddit/submissions/)中提取所有URL，

  - 使用Facebook的[fastText](https://github.com/facebookresearch/fastText)过滤掉非英语内容，

  - 删除近乎重复的内容，

#### 内容毒性得分

- OpenWebText有**2.1%**的内容毒性得分>=50%

- WebText有**4.3%**的内容毒性得分>=50%。
- **新闻的可靠性与毒性负相关**（Spearman ρ=−0.35），
- OpenWebText中有3%的内容来自被禁止或被隔离的subreddits，如/r/The_Donald和/r/WhiteRights。

### Colossal Clean Crawled Corpus（C4）

**[C4语料库](https://www.tensorflow.org/datasets/catalog/c4)被用来训练T5模型**

。这个语料库从2019年4月的Common Crawl快照（1.4万亿个标记）开始，移除了“[bad words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en)”，移除了代码（“{”），通过langdetect过滤掉了非英语文本，最终得到了806GB的文本（1560亿个标记）。

[Dodge等人](https://arxiv.org/pdf/2104.08758.pdf)在2021年对C4数据集进行了深入分析。分析主要涉及以下几个方面：

- 元数据：来源，话语数据。
- 包含的数据：由机器或人类创作的，社会偏见，数据污染。
- 排除的数据：医疗或健康数据，人口身份。
- 大量数据来自patents.google.com。
  - 互联网档案中的65%页面都被纳入其中，而在这些页面中，92%的页面是在过去十年内编写的 
  - 美国托管的页面占到了51.3%，

- 来自patents.google.com的一些文本是自动生成的，
  - 用外国的官方语言（如日语）提交的专利将自动翻译成英语；
  - 由光学字符识别（OCR）自动生成的。
    


### Benchmark的数据污染问题

评估大型语言模型的能力时，会使用一些基准数据，例如问题-答案对。然而，基准数据可能在模型的训练数据中出现过

以[XSum摘要](https://huggingface.co/datasets/xsum)数据集为例，存在两种类型的污染。

- 一种是输入和输出污染，即输入和输出都出现在训练数据中，其比例在1.87%至24.88%之间。

- 另一种是只有输入在训练数据中出现，比如来自维基百科的QNLI数据集，这种污染的比例在1.8%至53.6%之间。



### GPT-3的数据集



GPT-3的数据集主要源自Common Crawl，由于Common Crawl又类似于WebText，在处理数据时，采取了以下措施

- GPT-3采用了**模糊去重**的方法（检测13-gram重叠，如果在少于10个训练文档中出现，则移除窗口或文档），

- GPT-3也扩大了数据来源的多样性（包括WebText2、Books1、Books2以及维基百科）

- 在训练过程中，Common Crawl被降采样，它在数据集中占82%，但只贡献了60%的数据。

###  The Pile数据集

[The Pile](https://arxiv.org/pdf/2101.00027.pdf)数据集包含了825GB的英文文本，由22个高质量数据集组成

The Pile包含了大量GPT-3数据集未能很好覆盖的信息

网络和私有数据的总量是巨大的，但是简单地将所有数据都用于训练并不能有效地利用计算资源。数据的过滤和策划（如OpenWebText，C4，GPT-3数据集）是必要的



## 数据集文档

数据文档的主要目的有两个：

一方面，它**让数据集的创建者有机会反思他们的决策**，以及在**创建数据集过程中可能产生的潜在危害**，比如社会偏见；

另一方面，它**让数据集的使用者了解何时可以使用数据集，何时不应使用数据集。**

数据集的生命周期：

- 比如数据集的创建动机，
- 谁是数据集的创建者，
- 数据集的创建是由谁资助的。
- 在数据集的组成部分，
- 我们需要了解数据集中的实例代表什么，
- 是否有缺失信息，

收集过程中

- 是否包含机密数据
- 每个实例的数据是如何获取的，
- 谁参与了数据收集，
- 是如何获得报酬的，
- 是否进行了道德审查等

在预处理、清理和标记阶段，

- 工作是否已经完成，
- 是否有相应的软件可供使用。

在数据集的使用方面

- 需要了解数据集是否已经被用于某些任务，
- 是否有不适合使用该数据集的任务。

在分发阶段，

- 数据集将如何分发
- 是否有第三方对数据施加了知识产权或其他的限制。

在维护阶段

- 我们需要了解谁会负责维护数据集
- 数据集是否会更新。

## 数据生态

在数据管理方面，我们在机器学习研究中通常认为数据集是固定的对象，收集起来之后，直接投入到训练算法中。

数据库领域，正在**思考数据是如何产生和使用**的生态系统 

**数据尊严**是一个源自微软和RadicalxChange的概念，试图**思考数据的本质**。

人们创造数据，**数据也并不仅仅是个体的财产，而是群体的财产**。



