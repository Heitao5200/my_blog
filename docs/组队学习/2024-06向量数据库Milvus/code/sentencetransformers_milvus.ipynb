{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility\n",
    "from pymilvus import connections\n",
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
    "import csv\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import gdown\n",
    "# url = 'https://drive.google.com/uc?id=11ISS45aO2ubNCGaC3Lvd3D7NT8Y7MeO8'\n",
    "# output = './movies.zip'\n",
    "# gdown.download(url, output)\n",
    " \n",
    "# import zipfile\n",
    " \n",
    "# with zipfile.ZipFile(\"./movies.zip\",\"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"./movies\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全局参数\n",
    "\n",
    "在这里，我们可以找到需要修改以运行您自己的账户的主要参数。每个参数旁边都有一个描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milvus Setup Arguments\n",
    "COLLECTION_NAME = 'movies_db'  # Collection name\n",
    "DIMENSION = 384  # Embeddings size\n",
    "COUNT = 1000  # Number of vectors to insert\n",
    "MILVUS_HOST = 'localhost'\n",
    "MILVUS_PORT = '19530'\n",
    " \n",
    "# Inference Arguments\n",
    "BATCH_SIZE = 128\n",
    " \n",
    "# Search Arguments\n",
    "TOP_K = 3\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连接到 Milvus 数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除具有相同名称的以前集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建包含 ID、标题和情节文本嵌入的集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=200),  # VARCHAR 需要一个最大长度，所以为了这个例子，它们被设置为200个字符。\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "schema = CollectionSchema(fields=fields)\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为集合创建IVF_FLAT索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_params = {\n",
    "    'metric_type':'L2',\n",
    "    'index_type':\"IVF_FLAT\",\n",
    "    'params':{'nlist': 1536}\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "collection.load()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 01:19:07,029 - modelscope - INFO - PyTorch version 2.2.2 Found.\n",
      "2024-04-19 01:19:07,031 - modelscope - INFO - Loading ast index from C:\\Users\\mybcg\\.cache\\modelscope\\ast_indexer\n",
      "2024-04-19 01:19:07,130 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 61104cf01099cdfec0b7ca5a334bcfed and a total number of 972 components indexed\n",
      "Downloading: 100%|██████████| 698/698 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 86.7M/86.7M [00:04<00:00, 19.5MB/s]\n",
      "Downloading: 100%|██████████| 17.4k/17.4k [00:00<00:00, 3.55MB/s]\n",
      "Downloading: 100%|██████████| 695/695 [00:00<00:00, 684kB/s]\n",
      "Downloading: 100%|██████████| 695k/695k [00:00<00:00, 5.24MB/s]\n",
      "Downloading: 100%|██████████| 1.40k/1.40k [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 5.14MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('bensonpeng/all-MiniLM-L6-v2',cache_dir='./models')\n",
    "\n",
    "transformer = SentenceTransformer('./models/bensonpeng/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据到向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/bensonpeng/all-MiniLM-L6-v2. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def csv_load(file):\n",
    "    with open(file, newline='',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            if '' in (row[1], row[7]):\n",
    "                continue\n",
    "            yield (row[1], row[7])\n",
    " \n",
    "# Extract embeding from text using OpenAI\n",
    "def embed_insert(data):\n",
    "    embeds = transformer.encode(data[1]) \n",
    "    ins = [\n",
    "            data[0],\n",
    "            [x for x in embeds]\n",
    "    ]\n",
    "    collection.insert(ins)\n",
    " \n",
    "\n",
    "data_batch = [[],[]]\n",
    "count = 0\n",
    "\n",
    "for title, plot in csv_load('./wiki_movie_plots_deduped.csv'):\n",
    "    if count <= COUNT:\n",
    "        data_batch[0].append(title)\n",
    "        data_batch[1].append(plot)\n",
    "        if len(data_batch[0]) % BATCH_SIZE == 0:\n",
    "            embed_insert(data_batch)\n",
    "            data_batch = [[],[]]\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    " \n",
    "# Embed and insert the remainder\n",
    "if len(data_batch[0]) != 0:\n",
    "    embed_insert(data_batch)\n",
    " \n",
    "# Call a flush to index any unsealed segments.\n",
    "collection.flush()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将问题使用embedding转换成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for titles that closest match these phrases.\n",
    "search_terms = ['A movie about cars', 'A movie about monsters']\n",
    " \n",
    "# Search the database based on input text\n",
    "def embed_search(data):\n",
    "    embeds = transformer.encode(data) \n",
    "    return [x for x in embeds]\n",
    " \n",
    "search_data = embed_search(search_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A movie about cars\n",
      "Search Time: 0.01253509521484375\n",
      "Results:\n",
      "From Leadville to Aspen: A Hold-Up in the Rockies ---- 39.53708267211914\n",
      "Gentlemen of Nerve ---- 39.97119903564453\n",
      "Hot Water ---- 41.052330017089844\n",
      "\n",
      "Title: A movie about monsters\n",
      "Search Time: 0.01253509521484375\n",
      "Results:\n",
      "The Suburbanite ---- 39.47476577758789\n",
      "The Shriek of Araby ---- 42.2584228515625\n",
      "The Cavalier ---- 42.49919128417969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "start = time.time()\n",
    "res = collection.search(\n",
    "    data=search_data,  # Embeded search value\n",
    "    anns_field=\"embedding\",  # Search across embeddings\n",
    "    param={ \n",
    "                    },\n",
    "    limit = TOP_K,  # Limit to top_k results per search\n",
    "    output_fields=['title']  # Include title field in result\n",
    ")\n",
    "end = time.time()\n",
    " \n",
    "for hits_i, hits in enumerate(res):\n",
    "    print('Title:', search_terms[hits_i])\n",
    "    print('Search Time:', end-start)\n",
    "    print('Results:')\n",
    "    for hit in hits:\n",
    "        print( hit.entity.get('title'), '----', hit.distance)\n",
    "    print()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
