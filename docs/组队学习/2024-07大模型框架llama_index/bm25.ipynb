{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:09:58.847279Z",
     "start_time": "2024-04-13T12:09:32.958758Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /opt/anaconda3/envs/llm/lib/python3.10/site-\n",
      "[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /opt/anaconda3/envs/llm/lib/python3.10/site-\n",
      "[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "import logging\n",
    "import sys\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().handlers = []\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.legacy.retrievers.bm25_retriever import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.retrievers import RouterRetriever\n",
    "from llama_index.core.tools import RetrieverTool\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "# from llama_index.core.retrievers import BM25Retriever\n",
    "# from llama_index.llms import OpenAI, Ollama\n",
    "# from llama_index.embeddings import OllamaEmbedding\n",
    "\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "# from llama_index import QueryBundle\n",
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().handlers = []\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:10:04.150050Z",
     "start_time": "2024-04-13T12:10:04.141252Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdvancedRAG:\n",
    "    def __init__(self):\n",
    "        _ = load_dotenv(find_dotenv())\n",
    "        # load documents\n",
    "        self.documents = SimpleDirectoryReader(\"../data/\", required_exts=['.pdf']).load_data()\n",
    "\n",
    "        # global variables used later in code after initialization\n",
    "        self.retriever = None\n",
    "        self.reranker = None\n",
    "        self.query_engine = None\n",
    "\n",
    "        self.bootstrap()\n",
    "\n",
    "    def bootstrap(self):\n",
    "        # initialize LLMs\n",
    "        \n",
    "\n",
    "        llm = OpenAI(model=\"qwen\",\n",
    "                    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                    temperature=0,\n",
    "                    system_prompt=\"You are an expert on the Inflamatory Bowel Diseases and your job is to answer questions. Assume that all questions are related to the Inflammatory Bowel Diseases (IBD). Keep your answers technical and based on facts – do not hallucinate features.\")\n",
    "\n",
    "        # initialize service context (set chunk size)\n",
    "        service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)\n",
    "        nodes = service_context.node_parser.get_nodes_from_documents(self.documents)\n",
    "\n",
    "        # initialize storage context (by default it's in-memory)\n",
    "        storage_context = StorageContext.from_defaults()\n",
    "        storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "        index = VectorStoreIndex(\n",
    "            nodes=nodes,\n",
    "            storage_context=storage_context,\n",
    "            service_context=service_context,\n",
    "        )\n",
    "\n",
    "        # We can pass in the index, doctore, or list of nodes to create the retriever\n",
    "        self.retriever = BM25Retriever.from_defaults(similarity_top_k=2, index=index)\n",
    "\n",
    "        # reranker setup & initialization\n",
    "        self.reranker = SentenceTransformerRerank(top_n=1, model=\"/Users/heitao/my_blog/models/models/Xorbits/bge-reranker-base\")\n",
    "\n",
    "        self.query_engine = RetrieverQueryEngine.from_args(\n",
    "            retriever=self.retriever,\n",
    "            node_postprocessors=[self.reranker],\n",
    "            service_context=service_context,\n",
    "        )\n",
    "\n",
    "    def query(self, query):\n",
    "        # will retrieve context from specific companies\n",
    "        nodes = self.retriever.retrieve(query)\n",
    "        reranked_nodes = self.reranker.postprocess_nodes(\n",
    "            nodes,\n",
    "            query_bundle=QueryBundle(query_str=query)\n",
    "        )\n",
    "\n",
    "        print(\"Initial retrieval: \", len(nodes), \" nodes\")\n",
    "        print(\"Re-ranked retrieval: \", len(reranked_nodes), \" nodes\")\n",
    "\n",
    "        for node in nodes:\n",
    "            print(node)\n",
    "\n",
    "        for node in reranked_nodes:\n",
    "            print(node)\n",
    "\n",
    "        response = self.query_engine.query(str_or_query_bundle=query)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:10:09.022312Z",
     "start_time": "2024-04-13T12:10:05.905812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous trailer can not be read (\"invalid literal for int() with base 10: b'/Root'\",)\n",
      "Object 71 0 found\n",
      "Object 70 0 found\n",
      "Object 3 0 found\n",
      "Object 2 0 found\n",
      "Object 15 0 found\n",
      "Object 23 0 found\n",
      "Object 29 0 found\n",
      "Object 34 0 found\n",
      "Object 39 0 found\n",
      "Object 45 0 found\n",
      "Object 50 0 found\n",
      "Object 60 0 found\n",
      "Object 59 0 found\n",
      "Object 67 0 found\n",
      "Object 5 0 found\n",
      "Object 7 0 found\n",
      "Object 8 0 found\n",
      "Object 186 0 found\n",
      "Object 185 0 found\n",
      "Object 187 0 found\n",
      "Object 9 0 found\n",
      "Object 191 0 found\n",
      "Object 190 0 found\n",
      "Object 192 0 found\n",
      "Object 10 0 found\n",
      "Object 4 0 found\n",
      "Object 11 0 found\n",
      "Object 17 0 found\n",
      "Object 22 0 found\n",
      "Object 198 0 found\n",
      "Object 197 0 found\n",
      "Object 16 0 found\n",
      "Object 18 0 found\n",
      "Object 19 0 found\n",
      "Object 20 0 found\n",
      "Object 25 0 found\n",
      "Object 28 0 found\n",
      "Object 201 0 found\n",
      "Object 200 0 found\n",
      "Object 24 0 found\n",
      "Object 26 0 found\n",
      "Object 31 0 found\n",
      "Object 30 0 found\n",
      "Object 32 0 found\n",
      "Object 33 0 found\n",
      "Object 36 0 found\n",
      "Object 37 0 found\n",
      "Object 203 0 found\n",
      "Object 35 0 found\n",
      "Object 38 0 found\n",
      "Object 41 0 found\n",
      "Object 40 0 found\n",
      "Object 42 0 found\n",
      "Object 47 0 found\n",
      "Object 46 0 found\n",
      "Object 52 0 found\n",
      "Object 51 0 found\n",
      "Object 53 0 found\n",
      "Object 62 0 found\n",
      "Object 61 0 found\n",
      "Object 63 0 found\n",
      "Object 69 0 found\n",
      "Object 68 0 found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/dn__zk295kn6p0g2sr593nyr0000gn/T/ipykernel_58647/2535406003.py:24: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown model 'qwen'. Please provide a valid OpenAI model name in: gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4-turbo-preview, gpt-4-vision-preview, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo, gpt-35-turbo-0125, gpt-35-turbo-1106, gpt-35-turbo-0613, gpt-35-turbo-16k-0613",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     adv_rag \u001B[38;5;241m=\u001B[39m \u001B[43mAdvancedRAG\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     resp \u001B[38;5;241m=\u001B[39m adv_rag\u001B[38;5;241m.\u001B[39mquery(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTransformers are ?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(resp)\n",
      "Cell \u001B[0;32mIn[2], line 12\u001B[0m, in \u001B[0;36mAdvancedRAG.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreranker \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 24\u001B[0m, in \u001B[0;36mAdvancedRAG.bootstrap\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     18\u001B[0m llm \u001B[38;5;241m=\u001B[39m OpenAI(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mqwen\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     19\u001B[0m             api_key\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPENAI_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     20\u001B[0m             temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     21\u001B[0m             system_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are an expert on the Inflamatory Bowel Diseases and your job is to answer questions. Assume that all questions are related to the Inflammatory Bowel Diseases (IBD). Keep your answers technical and based on facts – do not hallucinate features.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# initialize service context (set chunk size)\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m service_context \u001B[38;5;241m=\u001B[39m \u001B[43mServiceContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_defaults\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m nodes \u001B[38;5;241m=\u001B[39m service_context\u001B[38;5;241m.\u001B[39mnode_parser\u001B[38;5;241m.\u001B[39mget_nodes_from_documents(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdocuments)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# initialize storage context (by default it's in-memory)\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/deprecated/classic.py:285\u001B[0m, in \u001B[0;36mdeprecated.<locals>.wrapper_function\u001B[0;34m(wrapped_, instance_, args_, kwargs_)\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    284\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(msg, category\u001B[38;5;241m=\u001B[39mcategory, stacklevel\u001B[38;5;241m=\u001B[39m_routine_stacklevel)\n\u001B[0;32m--> 285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs_\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/llama_index/core/service_context.py:204\u001B[0m, in \u001B[0;36mServiceContext.from_defaults\u001B[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001B[0m\n\u001B[1;32m    200\u001B[0m embed_model \u001B[38;5;241m=\u001B[39m resolve_embed_model(embed_model)\n\u001B[1;32m    201\u001B[0m embed_model\u001B[38;5;241m.\u001B[39mcallback_manager \u001B[38;5;241m=\u001B[39m callback_manager\n\u001B[1;32m    203\u001B[0m prompt_helper \u001B[38;5;241m=\u001B[39m prompt_helper \u001B[38;5;129;01mor\u001B[39;00m _get_default_prompt_helper(\n\u001B[0;32m--> 204\u001B[0m     llm_metadata\u001B[38;5;241m=\u001B[39m\u001B[43mllm_predictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m,\n\u001B[1;32m    205\u001B[0m     context_window\u001B[38;5;241m=\u001B[39mcontext_window,\n\u001B[1;32m    206\u001B[0m     num_output\u001B[38;5;241m=\u001B[39mnum_output,\n\u001B[1;32m    207\u001B[0m )\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_splitter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m node_parser \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot specify both text_splitter and node_parser\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/llama_index/core/service_context_elements/llm_predictor.py:157\u001B[0m, in \u001B[0;36mLLMPredictor.metadata\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmetadata\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMMetadata:\n\u001B[1;32m    156\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get LLM metadata.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 157\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_llm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/llama_index/llms/openai/base.py:281\u001B[0m, in \u001B[0;36mOpenAI.metadata\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmetadata\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMMetadata:\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m LLMMetadata(\n\u001B[0;32m--> 281\u001B[0m         context_window\u001B[38;5;241m=\u001B[39m\u001B[43mopenai_modelname_to_contextsize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_model_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    282\u001B[0m         num_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_tokens \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m    283\u001B[0m         is_chat_model\u001B[38;5;241m=\u001B[39mis_chat_model(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_model_name()),\n\u001B[1;32m    284\u001B[0m         is_function_calling_model\u001B[38;5;241m=\u001B[39mis_function_calling_model(\n\u001B[1;32m    285\u001B[0m             model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_model_name()\n\u001B[1;32m    286\u001B[0m         ),\n\u001B[1;32m    287\u001B[0m         model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel,\n\u001B[1;32m    288\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/llama_index/llms/openai/utils.py:203\u001B[0m, in \u001B[0;36mopenai_modelname_to_contextsize\u001B[0;34m(modelname)\u001B[0m\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpenAI model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodelname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has been discontinued. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    200\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease choose another model.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    201\u001B[0m     )\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m modelname \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ALL_AVAILABLE_MODELS:\n\u001B[0;32m--> 203\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    204\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodelname\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m. Please provide a valid OpenAI model name in:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    205\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(ALL_AVAILABLE_MODELS\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    206\u001B[0m     )\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ALL_AVAILABLE_MODELS[modelname]\n",
      "\u001B[0;31mValueError\u001B[0m: Unknown model 'qwen'. Please provide a valid OpenAI model name in: gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4-turbo-preview, gpt-4-vision-preview, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo, gpt-35-turbo-0125, gpt-35-turbo-1106, gpt-35-turbo-0613, gpt-35-turbo-16k-0613"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    adv_rag = AdvancedRAG()\n",
    "    resp = adv_rag.query(\"Transformers are ?\")\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:10:16.156050Z",
     "start_time": "2024-04-13T12:10:16.140237Z"
    }
   },
   "outputs": [],
   "source": [
    "model = \"qwen72chat\"\n",
    "url = \"http://220.250.27.14:8001/v1\"\n",
    "llm = OpenAI(model=\"qwen72chat\",\n",
    "            api_key='',\n",
    "            api_base = url,\n",
    "            temperature=0,\n",
    "            system_prompt=\"You are an expert on the Inflamatory Bowel Diseases and your job is to answer questions. Assume that all questions are related to the Inflammatory Bowel Diseases (IBD). Keep your answers technical and based on facts – do not hallucinate features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:10:18.497614Z",
     "start_time": "2024-04-13T12:10:18.392244Z"
    }
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChatStartEvent\nmessages\n  value is not a valid list (type=type_error.list)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m111\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py:129\u001B[0m, in \u001B[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001B[0;34m(_self, messages, **kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m wrapper_logic(_self) \u001B[38;5;28;01mas\u001B[39;00m callback_manager:\n\u001B[1;32m    127\u001B[0m     span_id \u001B[38;5;241m=\u001B[39m dispatcher\u001B[38;5;241m.\u001B[39mroot\u001B[38;5;241m.\u001B[39mcurrent_span_id \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    128\u001B[0m     dispatcher\u001B[38;5;241m.\u001B[39mevent(\n\u001B[0;32m--> 129\u001B[0m         \u001B[43mLLMChatStartEvent\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_self\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[43m            \u001B[49m\u001B[43madditional_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspan_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspan_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m     )\n\u001B[1;32m    136\u001B[0m     event_id \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_event_start(\n\u001B[1;32m    137\u001B[0m         CBEventType\u001B[38;5;241m.\u001B[39mLLM,\n\u001B[1;32m    138\u001B[0m         payload\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    142\u001B[0m         },\n\u001B[1;32m    143\u001B[0m     )\n\u001B[1;32m    144\u001B[0m     f_return_val \u001B[38;5;241m=\u001B[39m f(_self, messages, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001B[0m, in \u001B[0;36mBaseModel.__init__\u001B[0;34m(__pydantic_self__, **data)\u001B[0m\n\u001B[1;32m    339\u001B[0m values, fields_set, validation_error \u001B[38;5;241m=\u001B[39m validate_model(__pydantic_self__\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, data)\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validation_error:\n\u001B[0;32m--> 341\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m validation_error\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    343\u001B[0m     object_setattr(__pydantic_self__, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__dict__\u001B[39m\u001B[38;5;124m'\u001B[39m, values)\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for LLMChatStartEvent\nmessages\n  value is not a valid list (type=type_error.list)"
     ]
    }
   ],
   "source": [
    "llm.chat(messages = '111')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
