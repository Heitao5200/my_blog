{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723fcf40",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/retrievers/bm25_retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1de44-4047-46cf-a04c-dbf910d9e179",
   "metadata": {},
   "source": [
    "# BM25 Retriever\n",
    "In this guide, we define a bm25 retriever that search documents using bm25 method.\n",
    "\n",
    "This notebook is very similar to the RouterQueryEngine notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73fead-ec2c-4346-bd08-e183c13c7e29",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2630bc",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex 🦙."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d59778-4cda-47b5-8cd0-b80fee91d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04941476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c628448c-573c-4eeb-a7e1-707fe8cc575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().handlers = []\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.legacy.retrievers.bm25_retriever import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.retrievers import RouterRetriever\n",
    "from llama_index.core.tools import RetrieverTool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787174ed-10ce-47d7-82fd-9ca7f891eea7",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We first show how to convert a Document into a set of Nodes, and insert into a DocumentStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc1b8ac-bf55-4d60-841c-61698663322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7081194a-ede7-478e-bff2-23e89e23ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLM + node parser\n",
    "llm = OpenAI(model=\"zhipu\")\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f61bca2-c3b4-4ef0-a8f1-367933aa6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6162df-9da7-4aad-a2ca-eb318f67daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4b18c-4f9b-4311-b1d9-9cbf24687e2c",
   "metadata": {},
   "source": [
    "## BM25 Retriever\n",
    "\n",
    "We will search document with bm25 retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee3f7c3b-69b4-48d5-bf22-ac51a4e3179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass in the index, doctore, or list of nodes to create the retriever\n",
    "retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e49f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.schema import NodeWithScore, QueryBundle, Node\n",
    "\n",
    "# class JieRetriever(BM25Retriever):\n",
    "#     async def _get_scored_nodes(self, query: str):\n",
    "#         tokenized_query = self._tokenizer(query)\n",
    "#         doc_scores = self.bm25.get_scores(tokenized_query)\n",
    "#         nodes = []\n",
    "#         for i, node in enumerate(self._nodes):\n",
    "#             node_new = Node.from_dict(node.to_dict())\n",
    "#             node_with_score = NodeWithScore(node=node_new, score=doc_scores[i])\n",
    "#             nodes.append(node_with_score)\n",
    "#         return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7722e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAG:\n",
    "    def __init__(self):\n",
    "        _ = load_dotenv(find_dotenv())\n",
    "        # load documents\n",
    "        self.documents = SimpleDirectoryReader(\"./data/\", required_exts=['.pdf']).load_data()\n",
    "\n",
    "        # global variables used later in code after initialization\n",
    "        self.retriever = None\n",
    "        self.reranker = None\n",
    "        self.query_engine = None\n",
    "\n",
    "        self.bootstrap()\n",
    "\n",
    "    def bootstrap(self):\n",
    "        # initialize LLMs\n",
    "        llm = OpenAI(model=\"gpt-4\",\n",
    "                    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                    temperature=0,\n",
    "                    system_prompt=\"You are an expert on the Inflamatory Bowel Diseases and your job is to answer questions. Assume that all questions are related to the Inflammatory Bowel Diseases (IBD). Keep your answers technical and based on facts – do not hallucinate features.\")\n",
    "\n",
    "        # initialize service context (set chunk size)\n",
    "        service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)\n",
    "        nodes = service_context.node_parser.get_nodes_from_documents(self.documents)\n",
    "\n",
    "        # initialize storage context (by default it's in-memory)\n",
    "        storage_context = StorageContext.from_defaults()\n",
    "        storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "        index = VectorStoreIndex(\n",
    "            nodes=nodes,\n",
    "            storage_context=storage_context,\n",
    "            service_context=service_context,\n",
    "        )\n",
    "\n",
    "        # We can pass in the index, doctore, or list of nodes to create the retriever\n",
    "        self.retriever = BM25Retriever.from_defaults(similarity_top_k=2, index=index)\n",
    "\n",
    "        # reranker setup & initialization\n",
    "        self.reranker = SentenceTransformerRerank(top_n=1, model=\"BAAI/bge-reranker-base\")\n",
    "\n",
    "        self.query_engine = RetrieverQueryEngine.from_args(\n",
    "            retriever=self.retriever,\n",
    "            node_postprocessors=[self.reranker],\n",
    "            service_context=service_context,\n",
    "        )\n",
    "\n",
    "    def query(self, query):\n",
    "        # will retrieve context from specific companies\n",
    "        nodes = self.retriever.retrieve(query)\n",
    "        reranked_nodes = self.reranker.postprocess_nodes(\n",
    "            nodes,\n",
    "            query_bundle=QueryBundle(query_str=query)\n",
    "        )\n",
    "\n",
    "        print(\"Initial retrieval: \", len(nodes), \" nodes\")\n",
    "        print(\"Re-ranked retrieval: \", len(reranked_nodes), \" nodes\")\n",
    "\n",
    "        for node in nodes:\n",
    "            print(node)\n",
    "\n",
    "        for node in reranked_nodes:\n",
    "            print(node)\n",
    "\n",
    "        response = self.query_engine.query(str_or_query_bundle=query)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "452a08ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for NodeWithScore\nnode\n  Can't instantiate abstract class BaseNode with abstract methods get_content, get_metadata_str, get_type, hash, set_content (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the best way to learn Python?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_blog/codes/.venv/lib/python3.10/site-packages/llama_index/legacy/core/base_retriever.py:229\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    226\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[1;32m    227\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m    228\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[0;32m--> 229\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001b[1;32m    231\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    232\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[1;32m    233\u001b[0m         )\n",
      "File \u001b[0;32m~/my_blog/codes/.venv/lib/python3.10/site-packages/llama_index/legacy/retrievers/bm25_retriever.py:99\u001b[0m, in \u001b[0;36mBM25Retriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39mcustom_embedding_strs \u001b[38;5;129;01mor\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding:\n\u001b[1;32m     97\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25Retriever does not support embeddings, skipping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m scored_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scored_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Sort and get top_k nodes, score range => 0..1, closer to 1 means more relevant\u001b[39;00m\n\u001b[1;32m    102\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(scored_nodes, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/my_blog/codes/.venv/lib/python3.10/site-packages/llama_index/legacy/retrievers/bm25_retriever.py:91\u001b[0m, in \u001b[0;36mBM25Retriever._get_scored_nodes\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     89\u001b[0m nodes: List[NodeWithScore] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes):\n\u001b[0;32m---> 91\u001b[0m     nodes\u001b[38;5;241m.\u001b[39mappend(\u001b[43mNodeWithScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/my_blog/codes/.venv/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for NodeWithScore\nnode\n  Can't instantiate abstract class BaseNode with abstract methods get_content, get_metadata_str, get_type, hash, set_content (type=type_error)"
     ]
    }
   ],
   "source": [
    "retriever.retrieve(\"What is the best way to learn Python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b8c4c12-1a30-425e-8312-04be050b2101",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for NodeWithScore\nnode\n  Can't instantiate abstract class BaseNode with abstract methods get_content, get_metadata_str, get_type, hash, set_content (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display_source_node\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from llama_index.data_structs.node_v2 import BaseNode\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# will retrieve context from specific companies\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retriever\u001b[38;5;241m.\u001b[39maretrieve(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat happened at Viaweb and Interleaf?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[1;32m      6\u001b[0m     display_source_node(node)\n",
      "File \u001b[0;32m~/my_blog/codes/.venv/lib/python3.10/site-packages/llama_index/legacy/core/base_retriever.py:249\u001b[0m, in \u001b[0;36mBaseRetriever.aretrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    246\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[1;32m    247\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m    248\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[0;32m--> 249\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aretrieve(query_bundle)\n\u001b[1;32m    250\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ahandle_recursive_retrieval(query_bundle, nodes)\n\u001b[1;32m    251\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    252\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[1;32m    253\u001b[0m         )\n",
      "File \u001b[0;32m~/my_blog/codes/.venv/lib/python3.10/site-packages/llama_index/legacy/core/base_retriever.py:273\u001b[0m, in \u001b[0;36mBaseRetriever._aretrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously retrieve nodes given query.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Implemented by the user.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_blog/codes/.venv/lib/python3.10/site-packages/llama_index/legacy/retrievers/bm25_retriever.py:99\u001b[0m, in \u001b[0;36mBM25Retriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39mcustom_embedding_strs \u001b[38;5;129;01mor\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding:\n\u001b[1;32m     97\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25Retriever does not support embeddings, skipping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m scored_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scored_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Sort and get top_k nodes, score range => 0..1, closer to 1 means more relevant\u001b[39;00m\n\u001b[1;32m    102\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(scored_nodes, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/my_blog/codes/.venv/lib/python3.10/site-packages/llama_index/legacy/retrievers/bm25_retriever.py:91\u001b[0m, in \u001b[0;36mBM25Retriever._get_scored_nodes\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     89\u001b[0m nodes: List[NodeWithScore] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes):\n\u001b[0;32m---> 91\u001b[0m     nodes\u001b[38;5;241m.\u001b[39mappend(\u001b[43mNodeWithScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/my_blog/codes/.venv/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for NodeWithScore\nnode\n  Can't instantiate abstract class BaseNode with abstract methods get_content, get_metadata_str, get_type, hash, set_content (type=type_error)"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "# from llama_index.data_structs.node_v2 import BaseNode\n",
    "# will retrieve context from specific companies\n",
    "nodes = await retriever.aretrieve(\"What happened at Viaweb and Interleaf?\")\n",
    "for node in nodes:\n",
    "    display_source_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749c34e-97c0-4bd5-8358-377a94b8b2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 42c88008-dd05-4590-8fd1-df85567579ea<br>**Similarity:** 5.389397745654172<br>**Text:** It was missing a lot of things you'd want in a programming language. So these had to be added, an...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 4d98c2ad-60cc-4cc0-abe2-b95c0c4be87b<br>**Similarity:** 1.141523170922594<br>**Text:** Painting students were supposed to express themselves, which to the more worldly ones meant to tr...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"What did Paul Graham do after RISD?\")\n",
    "for node in nodes:\n",
    "    display_source_node(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73460e63-3c7a-49f4-80d2-ea9f4033848e",
   "metadata": {},
   "source": [
    "## Router Retriever with bm25 method\n",
    "\n",
    "Now we will combine bm25 retriever with vector index retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceedab8-c9e8-4ec9-be91-cba27482a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import RetrieverTool\n",
    "\n",
    "vector_retriever = VectorIndexRetriever(index)\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)\n",
    "\n",
    "retriever_tools = [\n",
    "    RetrieverTool.from_defaults(\n",
    "        retriever=vector_retriever,\n",
    "        description=\"Useful in most cases\",\n",
    "    ),\n",
    "    RetrieverTool.from_defaults(\n",
    "        retriever=bm25_retriever,\n",
    "        description=\"Useful if searching about specific information\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb850f9-c8b4-4843-b66c-8cb2a977c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import RouterRetriever\n",
    "\n",
    "retriever = RouterRetriever.from_defaults(\n",
    "    retriever_tools=retriever_tools,\n",
    "    llm=llm,\n",
    "    select_multi=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e29b3e-38e3-4f0b-a263-d267ed003cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Selecting retriever 0: The author's life context is a broad topic and can be useful in most cases..\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 9afb8cb9-42f3-4160-807c-1cd6685fa774<br>**Similarity:** 0.7961776744788842<br>**Text:** Now that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** b1ffb78d-dc4c-439b-906a-cca73b713940<br>**Similarity:** 0.7924813308773564<br>**Text:** We actually had one of those little stoves, fed with kindling, that you see in 19th century studi...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# will retrieve all context from the author's life\n",
    "nodes = retriever.retrieve(\n",
    "    \"Can you give me all the context regarding the author's life?\"\n",
    ")\n",
    "for node in nodes:\n",
    "    display_source_node(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4b717",
   "metadata": {},
   "source": [
    "## Advanced - Hybrid Retriever + Re-Ranking\n",
    "\n",
    "Here we extend the base retriever class and create a custom retriever that always uses the vector retriever and BM25 retreiver.\n",
    "\n",
    "Then, nodes can be re-ranked and filtered. This lets us keep intermediate top-k values large and letting the re-ranking filter out un-needed nodes.\n",
    "\n",
    "To best demonstrate this, we will use a larger set of source documents -- Chapter 3 from the 2022 IPCC Climate Report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be4ec7",
   "metadata": {},
   "source": [
    "### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd32ac-6d99-4bb5-8559-45b61d528525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader,\n",
    "    Document,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c771a80-92e3-4178-b4a3-1f03c489b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize llm + node parser\n",
    "# -- here, we set a smaller chunk size, to allow for more effective re-ranking\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "splitter = SentenceSplitter(chunk_size=256)\n",
    "# limit to a smaller section\n",
    "nodes = splitter.get_nodes_from_documents(\n",
    "    [Document(text=documents[0].get_content()[:1000000])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4243d-fd8f-4193-bf6a-73c07d661bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# retireve the top 10 most similar nodes using embeddings\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "# retireve the top 10 most similar nodes using bm25\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e603a1",
   "metadata": {},
   "source": [
    "### Custom Retriever Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        # combine the two lists of nodes\n",
    "        all_nodes = []\n",
    "        node_ids = set()\n",
    "        for n in bm25_nodes + vector_nodes:\n",
    "            if n.node.node_id not in node_ids:\n",
    "                all_nodes.append(n)\n",
    "                node_ids.add(n.node.node_id)\n",
    "        return all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495e47c-2008-41ac-87e8-582d6d798190",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3b287",
   "metadata": {},
   "source": [
    "### Re-Ranker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d910de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "reranker = SentenceTransformerRerank(top_n=4, model=\"BAAI/bge-reranker-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a78eec",
   "metadata": {},
   "source": [
    "### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "\n",
    "retrieved_nodes = hybrid_retriever.retrieve(\n",
    "    \"What is the impact of climate change on the ocean?\"\n",
    ")\n",
    "reranked_nodes = reranker.postprocess_nodes(\n",
    "    nodes,\n",
    "    query_bundle=QueryBundle(\n",
    "        \"What is the impact of climate change on the ocean?\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Initial retrieval: \", len(retrieved_nodes), \" nodes\")\n",
    "print(\"Re-ranked retrieval: \", len(reranked_nodes), \" nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5674f1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 735c563d-e68c-42e7-bbeb-d360a2918d22<br>**Similarity:** 0.6910567283630371<br>**Text:** lb88\\8a|hiac2:,sg\u001c\n",
       "\u001a`HfvyȔs\u001fkz4\u000b\n",
       "4+)t$EAs9\b<\"L\u0013䴺-iai]\u000f}?R)J\u0003\u0002\u0018v8Q0V#9>f\u001b=3`߼\bב&WTFK\u001bNӅ9GN8v`4׏g\f\n",
       "tz...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 07a796db-8307-47a0-83a0-9b5f953b86b0<br>**Similarity:** 0.609274685382843<br>**Text:** {_d3<,f\u0001CȀ\u001e\n",
       "0K0~n\u001e\n",
       "(\u0018A\t\b̙$saxt\f\n",
       " :\u001e\n",
       "xY3\u000fz/ߕXAW\u000fwpTeHY0\u0015\u0012H\u001bZHe\b̚kÇ>.\u001882%ϖ\u0017^.rCS.2^\u001a12C?<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 0ee98fbd-d015-4cd4-aba0-812423880915<br>**Similarity:** 0.5765283107757568<br>**Text:** X+V/Z\u001d\n",
       "\u000b\n",
       "6\u000e;\u0005O8/%Z3EC\\a\u0010sU1f(\u0016xLͼ]X\\\u0014\u0018q\"1}.66OKGSi\u0001\u000f9ǧ\u0015'(?Iʲi=4ޮ^m,Mp1~\u0013lBxP\u00153]\u0011S<?C)3WM;\u0016WGQt@\bl...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 2562bbfb-fda2-432d-9674-5ba77ff4b767<br>**Similarity:** 0.3882860541343689<br>**Text:** P\bxS0s\u0010.MAK|+MOUÊ\u0016^;tc0\n",
       "\u001a$\u0002XH\u0006mL\u0004<ҁ;Q@nUd&\u0013ի\n",
       "2`\u0018B0\u00041i^yOw:rsM冫S*wu4a{\u0001-.Խ=j鷩nQ\u0006D_A޾A%~T>~\u0018'OxU\tgk...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in reranked_nodes:\n",
    "    display_source_node(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24caa0d",
   "metadata": {},
   "source": [
    "### Full Query Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid_retriever,\n",
    "    node_postprocessors=[reranker],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What is the impact of climate change on the ocean?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171668b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Climate change can have a significant impact on the ocean. Rising temperatures can lead to the melting of polar ice caps, resulting in sea-level rise and coastal flooding. It can also disrupt ocean currents and affect marine ecosystems, including coral reefs and fish populations. Additionally, climate change can lead to ocean acidification, which can harm marine life and impact the overall health of the ocean."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "display_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
