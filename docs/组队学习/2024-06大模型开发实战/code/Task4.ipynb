{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ•°æ®å‡†å¤‡\n",
    "## æ•°æ®é›†ä¸‹è½½\n",
    "\n",
    "æˆ‘ä»¬é€‰ç”¨ Datawhale ä¸€äº›ç»å…¸å¼€æºè¯¾ç¨‹ä½œä¸ºç¤ºä¾‹ï¼Œå…·ä½“åŒ…æ‹¬ï¼š\n",
    "\n",
    "* [ã€Šæœºå™¨å­¦ä¹ å…¬å¼è¯¦è§£ã€‹PDFç‰ˆæœ¬](https://github.com/datawhalechina/pumpkin-book/releases)\n",
    "* [ã€Šé¢å‘å¼€å‘è€…çš„LLMå…¥é—¨æ•™ç¨‹ã€ç¬¬ä¸€éƒ¨åˆ†Prompt Engineeringã€‹mdç‰ˆæœ¬](https://github.com/datawhalechina/llm-cookbook)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-v1.0.0.pdf            LLM-v1.0.0_md.zip         pumpkin_book.pdf\n",
      "LLM-v1.0.0_latex.zip      LLM-v1.0.0_md_dollar.zip\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬æ•°æ®è§£æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª PyMuPDFLoader Class å®ä¾‹ï¼Œè¾“å…¥ä¸ºå¾…åŠ è½½çš„ pdf æ–‡æ¡£è·¯å¾„\n",
    "loader = PyMuPDFLoader(\"../data/pumpkin_book.pdf\")\n",
    "\n",
    "# è°ƒç”¨ PyMuPDFLoader Class çš„å‡½æ•° load å¯¹ pdf æ–‡ä»¶è¿›è¡ŒåŠ è½½\n",
    "pdf_pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¯ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼š<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "è¯¥æ–‡æ¡£çš„æè¿°æ€§æ•°æ®ï¼š{'source': '../data/pumpkin_book.pdf', 'file_path': '../data/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}\n",
      "------\n",
      "æŸ¥çœ‹è¯¥æ–‡æ¡£çš„å†…å®¹:\n",
      "å‰è¨€\n",
      "â€œå‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹\n",
      "ï¼ˆè¥¿ç“œä¹¦ï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ï¼Œå‘¨è€å¸ˆä¸ºäº†ä½¿å°½å¯èƒ½å¤šçš„è¯»\n",
      "è€…é€šè¿‡è¥¿ç“œä¹¦å¯¹æœºå™¨å­¦ä¹ æœ‰æ‰€äº†è§£, æ‰€ä»¥åœ¨ä¹¦ä¸­å¯¹éƒ¨åˆ†å…¬å¼çš„æ¨å¯¼ç»†èŠ‚æ²¡æœ‰è¯¦è¿°ï¼Œä½†æ˜¯è¿™å¯¹é‚£äº›æƒ³æ·±ç©¶å…¬å¼æ¨\n",
      "å¯¼ç»†èŠ‚çš„è¯»è€…æ¥è¯´å¯èƒ½â€œä¸å¤ªå‹å¥½â€\n",
      "ï¼Œæœ¬ä¹¦æ—¨åœ¨å¯¹è¥¿ç“œä¹¦é‡Œæ¯”è¾ƒéš¾ç†è§£çš„å…¬å¼åŠ ä»¥è§£æï¼Œä»¥åŠå¯¹éƒ¨åˆ†å…¬å¼è¡¥å……\n",
      "å…·ä½“çš„æ¨å¯¼ç»†èŠ‚ã€‚\n",
      "â€\n",
      "è¯»åˆ°è¿™é‡Œï¼Œå¤§å®¶å¯èƒ½ä¼šç–‘é—®ä¸ºå•¥å‰é¢è¿™æ®µè¯åŠ äº†å¼•å·ï¼Œå› ä¸ºè¿™åªæ˜¯æˆ‘ä»¬æœ€åˆçš„éæƒ³ï¼Œåæ¥æˆ‘ä»¬äº†è§£åˆ°ï¼Œå‘¨\n",
      "è€å¸ˆä¹‹æ‰€ä»¥çœå»è¿™äº›æ¨å¯¼ç»†èŠ‚çš„çœŸå®åŸå› æ˜¯ï¼Œä»–æœ¬å°Šè®¤ä¸ºâ€œç†å·¥ç§‘æ•°å­¦åŸºç¡€æ‰å®ç‚¹çš„å¤§äºŒä¸‹å­¦ç”Ÿåº”è¯¥å¯¹è¥¿ç“œä¹¦\n",
      "ä¸­çš„æ¨å¯¼ç»†èŠ‚æ— å›°éš¾å§ï¼Œè¦ç‚¹åœ¨ä¹¦é‡Œéƒ½æœ‰äº†ï¼Œç•¥å»çš„ç»†èŠ‚åº”èƒ½è„‘è¡¥æˆ–åšç»ƒä¹ â€\n",
      "ã€‚æ‰€ä»¥...... æœ¬å—ç“œä¹¦åªèƒ½ç®—æ˜¯æˆ‘\n",
      "ç­‰æ•°å­¦æ¸£æ¸£åœ¨è‡ªå­¦çš„æ—¶å€™è®°ä¸‹æ¥çš„ç¬”è®°ï¼Œå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©å¤§å®¶éƒ½æˆä¸ºä¸€ååˆæ ¼çš„â€œç†å·¥ç§‘æ•°å­¦åŸºç¡€æ‰å®ç‚¹çš„å¤§äºŒ\n",
      "ä¸‹å­¦ç”Ÿâ€\n",
      "ã€‚\n",
      "ä½¿ç”¨è¯´æ˜\n",
      "â€¢ å—ç“œä¹¦çš„æ‰€æœ‰å†…å®¹éƒ½æ˜¯ä»¥è¥¿ç“œä¹¦çš„å†…å®¹ä¸ºå‰ç½®çŸ¥è¯†è¿›è¡Œè¡¨è¿°çš„ï¼Œæ‰€ä»¥å—ç“œä¹¦çš„æœ€ä½³ä½¿ç”¨æ–¹æ³•æ˜¯ä»¥è¥¿ç“œä¹¦\n",
      "ä¸ºä¸»çº¿ï¼Œé‡åˆ°è‡ªå·±æ¨å¯¼ä¸å‡ºæ¥æˆ–è€…çœ‹ä¸æ‡‚çš„å…¬å¼æ—¶å†æ¥æŸ¥é˜…å—ç“œä¹¦ï¼›\n",
      "â€¢ å¯¹äºåˆå­¦æœºå™¨å­¦ä¹ çš„å°ç™½ï¼Œè¥¿ç“œä¹¦ç¬¬1 ç« å’Œç¬¬2 ç« çš„å…¬å¼å¼ºçƒˆä¸å»ºè®®æ·±ç©¶ï¼Œç®€å•è¿‡ä¸€ä¸‹å³å¯ï¼Œç­‰ä½ å­¦å¾—\n",
      "æœ‰ç‚¹é£˜çš„æ—¶å€™å†å›æ¥å•ƒéƒ½æ¥å¾—åŠï¼›\n",
      "â€¢ æ¯ä¸ªå…¬å¼çš„è§£æå’Œæ¨å¯¼æˆ‘ä»¬éƒ½åŠ›(zhi) äº‰(neng) ä»¥æœ¬ç§‘æ•°å­¦åŸºç¡€çš„è§†è§’è¿›è¡Œè®²è§£ï¼Œæ‰€ä»¥è¶…çº²çš„æ•°å­¦çŸ¥è¯†\n",
      "æˆ‘ä»¬é€šå¸¸éƒ½ä¼šä»¥é™„å½•å’Œå‚è€ƒæ–‡çŒ®çš„å½¢å¼ç»™å‡ºï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ç»§ç»­æ²¿ç€æˆ‘ä»¬ç»™çš„èµ„æ–™è¿›è¡Œæ·±å…¥å­¦ä¹ ï¼›\n",
      "â€¢ è‹¥å—ç“œä¹¦é‡Œæ²¡æœ‰ä½ æƒ³è¦æŸ¥é˜…çš„å…¬å¼ï¼Œ\n",
      "æˆ–è€…ä½ å‘ç°å—ç“œä¹¦å“ªä¸ªåœ°æ–¹æœ‰é”™è¯¯ï¼Œ\n",
      "è¯·æ¯«ä¸çŠ¹è±«åœ°å»æˆ‘ä»¬GitHub çš„\n",
      "Issuesï¼ˆåœ°å€ï¼šhttps://github.com/datawhalechina/pumpkin-book/issuesï¼‰è¿›è¡Œåé¦ˆï¼Œåœ¨å¯¹åº”ç‰ˆå—\n",
      "æäº¤ä½ å¸Œæœ›è¡¥å……çš„å…¬å¼ç¼–å·æˆ–è€…å‹˜è¯¯ä¿¡æ¯ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåœ¨24 å°æ—¶ä»¥å†…ç»™æ‚¨å›å¤ï¼Œè¶…è¿‡24 å°æ—¶æœªå›å¤çš„\n",
      "è¯å¯ä»¥å¾®ä¿¡è”ç³»æˆ‘ä»¬ï¼ˆå¾®ä¿¡å·ï¼šat-Sm1lesï¼‰\n",
      "ï¼›\n",
      "é…å¥—è§†é¢‘æ•™ç¨‹ï¼šhttps://www.bilibili.com/video/BV1Mh411e7VU\n",
      "åœ¨çº¿é˜…è¯»åœ°å€ï¼šhttps://datawhalechina.github.io/pumpkin-bookï¼ˆä»…ä¾›ç¬¬1 ç‰ˆï¼‰\n",
      "æœ€æ–°ç‰ˆPDF è·å–åœ°å€ï¼šhttps://github.com/datawhalechina/pumpkin-book/releases\n",
      "ç¼–å§”ä¼š\n",
      "ä¸»ç¼–ï¼šSm1lesã€archwalkerã€jbb0523\n",
      "ç¼–å§”ï¼šjuxiaoã€Majingminã€MrBigFanã€shanryã€Ye980226\n",
      "å°é¢è®¾è®¡ï¼šæ„æ€-Sm1lesã€åˆ›ä½œ-æ—ç‹èŒ‚ç››\n",
      "è‡´è°¢\n",
      "ç‰¹åˆ«æ„Ÿè°¢awyd234ã€\n",
      "feijuanã€\n",
      "Ggmatchã€\n",
      "Heitao5200ã€\n",
      "huaqing89ã€\n",
      "LongJHã€\n",
      "LilRachelã€\n",
      "LeoLRHã€\n",
      "Nono17ã€\n",
      "spareribsã€sunchaothuã€StevenLzq åœ¨æœ€æ—©æœŸçš„æ—¶å€™å¯¹å—ç“œä¹¦æ‰€åšçš„è´¡çŒ®ã€‚\n",
      "æ‰«æä¸‹æ–¹äºŒç»´ç ï¼Œç„¶åå›å¤å…³é”®è¯â€œå—ç“œä¹¦â€\n",
      "ï¼Œå³å¯åŠ å…¥â€œå—ç“œä¹¦è¯»è€…äº¤æµç¾¤â€\n",
      "ç‰ˆæƒå£°æ˜\n",
      "æœ¬ä½œå“é‡‡ç”¨çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº«4.0 å›½é™…è®¸å¯åè®®è¿›è¡Œè®¸å¯ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_page = pdf_pages[1]\n",
    "print(f\"æ¯ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼š{type(pdf_page)}.\", \n",
    "    f\"è¯¥æ–‡æ¡£çš„æè¿°æ€§æ•°æ®ï¼š{pdf_page.metadata}\", \n",
    "    f\"æŸ¥çœ‹è¯¥æ–‡æ¡£çš„å†…å®¹:\\n{pdf_page.page_content}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨[pdfdeal](https://github.com/Menghuan1918/pdfdeal?tab=readme-ov-file)åº“è¿›è¡Œpdfæ–‡ä»¶å¤„ç†(å¾…ä¼˜åŒ–)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdfdeal.doc2x import Doc2X\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# find_dotenv()\n",
    "# load_dotenv()\n",
    "# import os \n",
    "# Client = Doc2X()\n",
    "# file_type = 'pdf'\n",
    "# path = '../data/'\n",
    "# def gen_folder_list(path,file_type):\n",
    "\n",
    "#     for root, dirs, files in os.walk(path):\n",
    "#         # print(root,dirs,files)\n",
    "#         pdf_list = []\n",
    "#         for file in files:\n",
    "#             if file.endswith(f'.{file_type}'):\n",
    "#                 # print(os.path.join(root,file))\n",
    "#                 pdf_list.append(os.path.join(root,file))\n",
    "#     return pdf_list\n",
    "# filelist = gen_folder_list(path,file_type)\n",
    "# # This is a built-in function for generating the folder under the path of all the pdf, you can give any list of the form of the path of the pdf\n",
    "# Client.pdfdeal(filelist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®æ¸…æ´—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='å‰è¨€\\nâ€œå‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ï¼Œå‘¨è€å¸ˆä¸ºäº†ä½¿å°½å¯èƒ½å¤šçš„è¯»\\nè€…é€šè¿‡è¥¿ç“œä¹¦å¯¹æœºå™¨å­¦ä¹ æœ‰æ‰€äº†è§£,æ‰€ä»¥åœ¨ä¹¦ä¸­å¯¹éƒ¨åˆ†å…¬å¼çš„æ¨å¯¼ç»†èŠ‚æ²¡æœ‰è¯¦è¿°ï¼Œä½†æ˜¯è¿™å¯¹é‚£äº›æƒ³æ·±ç©¶å…¬å¼æ¨\\nå¯¼ç»†èŠ‚çš„è¯»è€…æ¥è¯´å¯èƒ½â€œä¸å¤ªå‹å¥½â€ï¼Œæœ¬ä¹¦æ—¨åœ¨å¯¹è¥¿ç“œä¹¦é‡Œæ¯”è¾ƒéš¾ç†è§£çš„å…¬å¼åŠ ä»¥è§£æï¼Œä»¥åŠå¯¹éƒ¨åˆ†å…¬å¼è¡¥å……\\nå…·ä½“çš„æ¨å¯¼ç»†èŠ‚ã€‚â€\\nè¯»åˆ°è¿™é‡Œï¼Œå¤§å®¶å¯èƒ½ä¼šç–‘é—®ä¸ºå•¥å‰é¢è¿™æ®µè¯åŠ äº†å¼•å·ï¼Œå› ä¸ºè¿™åªæ˜¯æˆ‘ä»¬æœ€åˆçš„éæƒ³ï¼Œåæ¥æˆ‘ä»¬äº†è§£åˆ°ï¼Œå‘¨\\nè€å¸ˆä¹‹æ‰€ä»¥çœå»è¿™äº›æ¨å¯¼ç»†èŠ‚çš„çœŸå®åŸå› æ˜¯ï¼Œä»–æœ¬å°Šè®¤ä¸ºâ€œç†å·¥ç§‘æ•°å­¦åŸºç¡€æ‰å®ç‚¹çš„å¤§äºŒä¸‹å­¦ç”Ÿåº”è¯¥å¯¹è¥¿ç“œä¹¦\\nä¸­çš„æ¨å¯¼ç»†èŠ‚æ— å›°éš¾å§ï¼Œè¦ç‚¹åœ¨ä¹¦é‡Œéƒ½æœ‰äº†ï¼Œç•¥å»çš„ç»†èŠ‚åº”èƒ½è„‘è¡¥æˆ–åšç»ƒä¹ â€ã€‚æ‰€ä»¥......æœ¬å—ç“œä¹¦åªèƒ½ç®—æ˜¯æˆ‘\\nç­‰æ•°å­¦æ¸£æ¸£åœ¨è‡ªå­¦çš„æ—¶å€™è®°ä¸‹æ¥çš„ç¬”è®°ï¼Œå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©å¤§å®¶éƒ½æˆä¸ºä¸€ååˆæ ¼çš„â€œç†å·¥ç§‘æ•°å­¦åŸºç¡€æ‰å®ç‚¹çš„å¤§äºŒ\\nä¸‹å­¦ç”Ÿâ€ã€‚\\nä½¿ç”¨è¯´æ˜\\nå—ç“œä¹¦çš„æ‰€æœ‰å†…å®¹éƒ½æ˜¯ä»¥è¥¿ç“œä¹¦çš„å†…å®¹ä¸ºå‰ç½®çŸ¥è¯†è¿›è¡Œè¡¨è¿°çš„ï¼Œæ‰€ä»¥å—ç“œä¹¦çš„æœ€ä½³ä½¿ç”¨æ–¹æ³•æ˜¯ä»¥è¥¿ç“œä¹¦\\nä¸ºä¸»çº¿ï¼Œé‡åˆ°è‡ªå·±æ¨å¯¼ä¸å‡ºæ¥æˆ–è€…çœ‹ä¸æ‡‚çš„å…¬å¼æ—¶å†æ¥æŸ¥é˜…å—ç“œä¹¦ï¼›å¯¹äºåˆå­¦æœºå™¨å­¦ä¹ çš„å°ç™½ï¼Œè¥¿ç“œä¹¦ç¬¬1ç« å’Œç¬¬2ç« çš„å…¬å¼å¼ºçƒˆä¸å»ºè®®æ·±ç©¶ï¼Œç®€å•è¿‡ä¸€ä¸‹å³å¯ï¼Œç­‰ä½ å­¦å¾—\\næœ‰ç‚¹é£˜çš„æ—¶å€™å†å›æ¥å•ƒéƒ½æ¥å¾—åŠï¼›æ¯ä¸ªå…¬å¼çš„è§£æå’Œæ¨å¯¼æˆ‘ä»¬éƒ½åŠ›(zhi)äº‰(neng)ä»¥æœ¬ç§‘æ•°å­¦åŸºç¡€çš„è§†è§’è¿›è¡Œè®²è§£ï¼Œæ‰€ä»¥è¶…çº²çš„æ•°å­¦çŸ¥è¯†\\næˆ‘ä»¬é€šå¸¸éƒ½ä¼šä»¥é™„å½•å’Œå‚è€ƒæ–‡çŒ®çš„å½¢å¼ç»™å‡ºï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ç»§ç»­æ²¿ç€æˆ‘ä»¬ç»™çš„èµ„æ–™è¿›è¡Œæ·±å…¥å­¦ä¹ ï¼›è‹¥å—ç“œä¹¦é‡Œæ²¡æœ‰ä½ æƒ³è¦æŸ¥é˜…çš„å…¬å¼ï¼Œ\\næˆ–è€…ä½ å‘ç°å—ç“œä¹¦å“ªä¸ªåœ°æ–¹æœ‰é”™è¯¯ï¼Œ\\nè¯·æ¯«ä¸çŠ¹è±«åœ°å»æˆ‘ä»¬GitHubçš„\\nIssuesï¼ˆåœ°å€ï¼šhttps://github.com/datawhalechina/pumpkin-book/issuesï¼‰è¿›è¡Œåé¦ˆï¼Œåœ¨å¯¹åº”ç‰ˆå—\\næäº¤ä½ å¸Œæœ›è¡¥å……çš„å…¬å¼ç¼–å·æˆ–è€…å‹˜è¯¯ä¿¡æ¯ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåœ¨24å°æ—¶ä»¥å†…ç»™æ‚¨å›å¤ï¼Œè¶…è¿‡24å°æ—¶æœªå›å¤çš„\\nè¯å¯ä»¥å¾®ä¿¡è”ç³»æˆ‘ä»¬ï¼ˆå¾®ä¿¡å·ï¼šat-Sm1lesï¼‰ï¼›\\né…å¥—è§†é¢‘æ•™ç¨‹ï¼šhttps://www.bilibili.com/video/BV1Mh411e7VU\\nåœ¨çº¿é˜…è¯»åœ°å€ï¼šhttps://datawhalechina.github.io/pumpkin-bookï¼ˆä»…ä¾›ç¬¬1ç‰ˆï¼‰\\næœ€æ–°ç‰ˆPDFè·å–åœ°å€ï¼šhttps://github.com/datawhalechina/pumpkin-book/releases\\nç¼–å§”ä¼š\\nä¸»ç¼–ï¼šSm1lesã€archwalkerã€jbb0523\\nç¼–å§”ï¼šjuxiaoã€Majingminã€MrBigFanã€shanryã€Ye980226\\nå°é¢è®¾è®¡ï¼šæ„æ€-Sm1lesã€åˆ›ä½œ-æ—ç‹èŒ‚ç››\\nè‡´è°¢\\nç‰¹åˆ«æ„Ÿè°¢awyd234ã€feijuanã€Ggmatchã€Heitao5200ã€huaqing89ã€LongJHã€LilRachelã€LeoLRHã€Nono17ã€spareribsã€sunchaothuã€StevenLzqåœ¨æœ€æ—©æœŸçš„æ—¶å€™å¯¹å—ç“œä¹¦æ‰€åšçš„è´¡çŒ®ã€‚\\næ‰«æä¸‹æ–¹äºŒç»´ç ï¼Œç„¶åå›å¤å…³é”®è¯â€œå—ç“œä¹¦â€ï¼Œå³å¯åŠ å…¥â€œå—ç“œä¹¦è¯»è€…äº¤æµç¾¤â€\\nç‰ˆæƒå£°æ˜\\næœ¬ä½œå“é‡‡ç”¨çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº«4.0å›½é™…è®¸å¯åè®®è¿›è¡Œè®¸å¯ã€‚\\n', metadata={'source': '../data/pumpkin_book.pdf', 'file_path': '../data/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "def clearn_page_content(pdf_page):\n",
    "\n",
    "    pdf_page.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), pdf_page.page_content)\n",
    "    pdf_page.page_content = pdf_page.page_content.replace('â€¢', '')\n",
    "    pdf_page.page_content = pdf_page.page_content.replace(' ', '')\n",
    "    # print(pdf_page.page_content)\n",
    "    return pdf_page\n",
    "clearn_page_content(pdf_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬åˆ‡åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çŸ¥è¯†åº“ä¸­å•æ®µæ–‡æœ¬é•¿åº¦\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "# çŸ¥è¯†åº“ä¸­ç›¸é‚»æ–‡æœ¬é‡åˆé•¿åº¦\n",
    "OVERLAP_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ‡åˆ†åçš„æ–‡ä»¶æ•°é‡ï¼š720\n",
      "åˆ‡åˆ†åçš„å­—ç¬¦æ•°ï¼ˆå¯ä»¥ç”¨æ¥å¤§è‡´è¯„ä¼° token æ•°ï¼‰ï¼š308791\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "split_docs = text_splitter.split_documents(pdf_pages)\n",
    "print(f\"åˆ‡åˆ†åçš„æ–‡ä»¶æ•°é‡ï¼š{len(split_docs)}\")\n",
    "\n",
    "print(f\"åˆ‡åˆ†åçš„å­—ç¬¦æ•°ï¼ˆå¯ä»¥ç”¨æ¥å¤§è‡´è¯„ä¼° token æ•°ï¼‰ï¼š{sum([len(doc.page_content) for doc in split_docs])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹æ­å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "_ = load_dotenv(find_dotenv())    # read local .env file\n",
    "zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai_llm import ZhipuAILLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ZhipuAILLM(temperature=0,api_key=os.environ['ZHIPUAI_API_KEY'])\n",
    "llm = ZhipuAILLM(\n",
    "    model = 'glm-4',\n",
    "    max_tokens = 256,\n",
    "    temperature = 0.8,\n",
    "    api_key=os.environ['ZHIPUAI_API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('ä½ å¥½')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milvuså‘é‡åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility\n",
    "from pymilvus import connections\n",
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = 'rag_db'  # Collection name\n",
    "DIMENSION = 768  # Embeddings size\n",
    "COUNT = 1000  # Number of vectors to insert\n",
    "MILVUS_HOST = 'localhost'\n",
    "MILVUS_PORT = '19530' # Inference Arguments\n",
    "BATCH_SIZE = 128\n",
    "MAX_LENGTH = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document2df(pdf_pages):\n",
    "    # ç”¨äºè·å–å…ƒæ•°æ® å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“è¿›è¡Œæ··åˆå‘é‡æŸ¥è¯¢\n",
    "    df_list = []\n",
    "    for pdf_page in pdf_pages:\n",
    "        pdf_page = clearn_page_content(pdf_page)\n",
    "        pdf_page_dict = pdf_page.dict().get('metadata')\n",
    "        pdf_page_dict.update({'page_content':pdf_page.dict().get('page_content')})\n",
    "        df = pd.json_normalize(pdf_page_dict)\n",
    "        df_list.append(df)\n",
    "    df = pd.concat(df_list, ignore_index=True).reset_index()  \n",
    "    return df\n",
    "df = document2df(pdf_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>source</th>\n",
       "      <th>file_path</th>\n",
       "      <th>page</th>\n",
       "      <th>total_pages</th>\n",
       "      <th>format</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>subject</th>\n",
       "      <th>keywords</th>\n",
       "      <th>creator</th>\n",
       "      <th>producer</th>\n",
       "      <th>creationDate</th>\n",
       "      <th>modDate</th>\n",
       "      <th>trapped</th>\n",
       "      <th>page_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>PDF 1.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LaTeX with hyperref</td>\n",
       "      <td>xdvipdfmx (20200315)</td>\n",
       "      <td>D:20231117152045-00'00'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\u0001æœ¬\u0003:2.0.0\\nå‘å¸ƒæ—¥æœŸ:2023.11\\nå—â½ ä¹¦\\nPUMPKINBOOK\\nè°¢\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>PDF 1.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LaTeX with hyperref</td>\n",
       "      <td>xdvipdfmx (20200315)</td>\n",
       "      <td>D:20231117152045-00'00'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>å‰è¨€\\nâ€œå‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ï¼Œå‘¨è€å¸ˆä¸ºäº†ä½¿å°½...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>196</td>\n",
       "      <td>PDF 1.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LaTeX with hyperref</td>\n",
       "      <td>xdvipdfmx (20200315)</td>\n",
       "      <td>D:20231117152045-00'00'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>â†’_â†’\\næ¬¢è¿å»å„å¤§ç”µå•†å¹³å°é€‰è´­çº¸è´¨ç‰ˆå—ç“œä¹¦ã€Šæœºå™¨å­¦ä¹ å…¬å¼è¯¦è§£ç¬¬2ç‰ˆã€‹â†_â†\\nç›®å½•\\nç¬¬1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>196</td>\n",
       "      <td>PDF 1.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LaTeX with hyperref</td>\n",
       "      <td>xdvipdfmx (20200315)</td>\n",
       "      <td>D:20231117152045-00'00'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>â†’_â†’\\næ¬¢è¿å»å„å¤§ç”µå•†å¹³å°é€‰è´­çº¸è´¨ç‰ˆå—ç“œä¹¦ã€Šæœºå™¨å­¦ä¹ å…¬å¼è¯¦è§£ç¬¬2ç‰ˆã€‹â†_â†3.3.1\\nå¼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>../data/pumpkin_book.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>196</td>\n",
       "      <td>PDF 1.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LaTeX with hyperref</td>\n",
       "      <td>xdvipdfmx (20200315)</td>\n",
       "      <td>D:20231117152045-00'00'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>â†’_â†’\\næ¬¢è¿å»å„å¤§ç”µå•†å¹³å°é€‰è´­çº¸è´¨ç‰ˆå—ç“œä¹¦ã€Šæœºå™¨å­¦ä¹ å…¬å¼è¯¦è§£ç¬¬2ç‰ˆã€‹â†_â†5.5\\nå…¶ä»–å¸¸...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                    source                 file_path  page  \\\n",
       "0      0  ../data/pumpkin_book.pdf  ../data/pumpkin_book.pdf     0   \n",
       "1      1  ../data/pumpkin_book.pdf  ../data/pumpkin_book.pdf     1   \n",
       "2      2  ../data/pumpkin_book.pdf  ../data/pumpkin_book.pdf     2   \n",
       "3      3  ../data/pumpkin_book.pdf  ../data/pumpkin_book.pdf     3   \n",
       "4      4  ../data/pumpkin_book.pdf  ../data/pumpkin_book.pdf     4   \n",
       "\n",
       "   total_pages   format title author subject keywords              creator  \\\n",
       "0          196  PDF 1.5                                LaTeX with hyperref   \n",
       "1          196  PDF 1.5                                LaTeX with hyperref   \n",
       "2          196  PDF 1.5                                LaTeX with hyperref   \n",
       "3          196  PDF 1.5                                LaTeX with hyperref   \n",
       "4          196  PDF 1.5                                LaTeX with hyperref   \n",
       "\n",
       "               producer             creationDate modDate trapped  \\\n",
       "0  xdvipdfmx (20200315)  D:20231117152045-00'00'                   \n",
       "1  xdvipdfmx (20200315)  D:20231117152045-00'00'                   \n",
       "2  xdvipdfmx (20200315)  D:20231117152045-00'00'                   \n",
       "3  xdvipdfmx (20200315)  D:20231117152045-00'00'                   \n",
       "4  xdvipdfmx (20200315)  D:20231117152045-00'00'                   \n",
       "\n",
       "                                        page_content  \n",
       "0  \u0001æœ¬\u0003:2.0.0\\nå‘å¸ƒæ—¥æœŸ:2023.11\\nå—â½ ä¹¦\\nPUMPKINBOOK\\nè°¢\\t...  \n",
       "1  å‰è¨€\\nâ€œå‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ï¼Œå‘¨è€å¸ˆä¸ºäº†ä½¿å°½...  \n",
       "2  â†’_â†’\\næ¬¢è¿å»å„å¤§ç”µå•†å¹³å°é€‰è´­çº¸è´¨ç‰ˆå—ç“œä¹¦ã€Šæœºå™¨å­¦ä¹ å…¬å¼è¯¦è§£ç¬¬2ç‰ˆã€‹â†_â†\\nç›®å½•\\nç¬¬1...  \n",
       "3  â†’_â†’\\næ¬¢è¿å»å„å¤§ç”µå•†å¹³å°é€‰è´­çº¸è´¨ç‰ˆå—ç“œä¹¦ã€Šæœºå™¨å­¦ä¹ å…¬å¼è¯¦è§£ç¬¬2ç‰ˆã€‹â†_â†3.3.1\\nå¼...  \n",
       "4  â†’_â†’\\næ¬¢è¿å»å„å¤§ç”µå•†å¹³å°é€‰è´­çº¸è´¨ç‰ˆå—ç“œä¹¦ã€Šæœºå™¨å­¦ä¹ å…¬å¼è¯¦è§£ç¬¬2ç‰ˆã€‹â†_â†5.5\\nå…¶ä»–å¸¸...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_collection(COLLECTION_NAME):\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "delete_collection(COLLECTION_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_collection(collection_name):\n",
    " \n",
    "    # ä¸»é”®\n",
    "    field_id = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True)\n",
    "    # å‘é‡æ£€ç´¢çš„field\n",
    "    field_title = FieldSchema(name='page', dtype=DataType.INT64,  description ='page', max_length=MAX_LENGTH )\n",
    "    field_origin = FieldSchema(name='page_content', dtype=DataType.VARCHAR, description ='page_content' , max_length=8192 )\n",
    "\n",
    "    field_title_embedding = FieldSchema(name='page_content_embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION,description ='page_content' )\n",
    "    # field_plot_embedding = FieldSchema(name='plot_embedding', dtype=DataType.FLOAT_VECTOR,dim=64,description ='Plot' )\n",
    "    schema = CollectionSchema(fields=[field_id, \n",
    "                                      field_title, \n",
    "                                      field_origin,\n",
    "                              \n",
    "                                      field_title_embedding,\n",
    "                                      # field_plot_embedding\n",
    "                                     ], description=\"page_content_collection\")\n",
    "\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "   \n",
    "\n",
    "    return collection\n",
    "  \n",
    "\n",
    "collection = create_collection(COLLECTION_NAME)\n",
    "### ä¸ºé›†åˆåˆ›å»ºIVF_FLATç´¢å¼•\n",
    "def create_index_collection(collection):\n",
    "\n",
    "    \n",
    "    index_params = {\n",
    "        'metric_type':'L2',\n",
    "        'index_type':\"IVF_FLAT\",\n",
    "        'params':{'nlist': 1536}\n",
    "    }\n",
    "    collection.create_index(field_name=\"page_content_embedding\", index_params=index_params)\n",
    "    collection.load()\n",
    "create_index_collection(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¯»å–æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer = SentenceTransformer('/Users/heitao/models/AI-ModelScope/bge-base-zh-v1-5', )\n",
    "# transformer.encode(x)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸ºcollectionåˆ›å»ºåˆ†åŒº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\":\"_default\",\"collection_name\":\"rag_db\",\"description\":\"\"}, {\"name\":\"partition_test\",\"collection_name\":\"rag_db\",\"description\":\"\"}]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def create_partition(collection,partition_name):\n",
    "    \"\"\"\n",
    "    ä¸ºcollectionåˆ›å»ºåˆ†åŒº\n",
    "    :param collection:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    partition = collection.create_partition(partition_name)\n",
    "    print(collection.partitions)\n",
    "    print(collection.has_partition(partition_name))\n",
    "    \n",
    "create_partition(collection,partition_name = 'partition_test')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ’å…¥æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_collection(df ):\n",
    "    \n",
    "    field_page = df['page'].to_list()\n",
    "    field_page_content = df['page_content'].to_list()\n",
    "\n",
    "    sentences = df['page_content'].to_list()\n",
    "    embeddings = transformer.encode(sentences)\n",
    "    ins = [ field_page,field_page_content,embeddings]\n",
    "    collection.insert(ins)\n",
    "    collection.flush()\n",
    "insert_data_collection(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_query(search_terms):\n",
    "    embeds = transformer.encode(search_terms) \n",
    "    return [x for x in embeds]\n",
    "# Search for titles that closest match these phrases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(search_terms, top_k=3):\n",
    "    \n",
    "    search_data = embedding_query(search_terms)\n",
    "    res = collection.search(\n",
    "        data=search_data,  # Embeded search value\n",
    "        anns_field=\"page_content_embedding\",  # Search across embeddings\n",
    "        param={\n",
    "                # \"nprobe\": 128,\n",
    "                # \"metric_type\": \"L2\",\n",
    "                # \"offset\": 10,\n",
    "                # \"limit\": 10,\n",
    "                        },\n",
    "        limit = top_k,  # Limit to top_k results per search\n",
    "        output_fields=['page_content']  # Include title field in result\n",
    "    )\n",
    "    result = []\n",
    "    for hits_i, hits in enumerate(res):\n",
    "        print('Title:', search_terms[hits_i])\n",
    "        # print('Search Time:', end-start)\n",
    "        # print('Results:')\n",
    "        for hit in hits:\n",
    "            # print( hit.entity.get('page_content'), '----', hit.distance)\n",
    "            result.append(hit.entity.get('page_content'))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ„å»ºæ£€ç´¢é—®ç­”é“¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªå·±å®šä¹‰ä¸€ä¸ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: å­™æ‚Ÿç©ºçš„å¸ˆå‚…æ˜¯è°\n",
      "æˆ‘ä¸çŸ¥é“å­™æ‚Ÿç©ºçš„å¸ˆå‚…æ˜¯è°ï¼Œå› ä¸ºä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰æåˆ°ç›¸å…³ä¿¡æ¯ã€‚è°¢è°¢ä½ çš„æé—®ï¼\n"
     ]
    }
   ],
   "source": [
    "def rag(question):\n",
    "    search_terms = [question]\n",
    "    result = get_result(search_terms)\n",
    "    template = \"\"\"ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœæ‰€ç»™çš„ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰æåˆ°ç›¸å…³çš„ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”\n",
    "    æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚\n",
    "    ä¸Šä¸‹æ–‡å†…å®¹ï¼š{}\n",
    "    é—®é¢˜: {}\n",
    "    \"\"\".format('\\n'.join(result),search_terms[0])\n",
    "\n",
    "    answer = llm(template)\n",
    "    print(answer)\n",
    "    return answer\n",
    "question = 'å­™æ‚Ÿç©ºçš„å¸ˆå‚…æ˜¯è°'\n",
    "answer = rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores.milvus import Milvus\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='/Users/heitao/models/AI-ModelScope/bge-base-zh-v1-5')\n",
    "\n",
    "loader = PyPDFLoader(\"../data/pumpkin_book.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split docs\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# Insert the documents in Milvus Vector Store\n",
    "vector_db = Milvus.from_documents(\n",
    "    docs,\n",
    "    embedding_model,\n",
    "    collection_name='test',\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT},\n",
    "    )\n",
    "\n",
    "\n",
    "template = \"\"\"ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”\n",
    "æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚\n",
    "{context}\n",
    "é—®é¢˜: {question}\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vector_db.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "question_1 = \"ä»€ä¹ˆæ˜¯å—ç“œä¹¦ï¼Ÿ\"\n",
    "question_2 = \"ç‹é˜³æ˜æ˜¯è°ï¼Ÿ\"\n",
    "\n",
    "result = qa_chain({\"query\": question_1})\n",
    "print(\"å¤§æ¨¡å‹+çŸ¥è¯†åº“åå›ç­” question_1 çš„ç»“æœï¼š\")\n",
    "print(result[\"result\"])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 11:41:49.968 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/llm/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "st.title('ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘')\n",
    "zhipuai_api_key = st.sidebar.text_input('ZhiPu API Key', type='password')\n",
    "def generate_response(input_text):\n",
    "    llm = ChatOpenAI(temperature=0.7, openai_api_key=zhipuai_api_key)\n",
    "    st.info(llm(input_text))\n",
    "with st.form('my_form'):\n",
    "    text = st.text_area('Enter text:', 'What are the three key pieces of advice for learning how to code?')\n",
    "    submitted = st.form_submit_button('Submit')\n",
    "    if not zhipuai_api_key.startswith('sk-'):\n",
    "        st.warning('Please enter your OpenAI API key!', icon='âš ')\n",
    "    if submitted and zhipuai_api_key.startswith('sk-'):\n",
    "        generate_response(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import sys\n",
    "sys.path.append(\"../C3 æ­å»ºçŸ¥è¯†åº“\") # å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())    # read local .env file\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "from langchain.vectorstores.milvus import Milvus\n",
    "\n",
    "#export OPENAI_API_KEY=\n",
    "#os.environ[\"OPENAI_API_BASE\"] = 'https://api.chatgptid.net/v1'\n",
    "zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']\n",
    "embedding = HuggingFaceEmbeddings(model_name='/Users/heitao/models/AI-ModelScope/bge-base-zh-v1-5')\n",
    "loader = PyPDFLoader(\"../data/pumpkin_book.pdf\")\n",
    "data = loader.load()\n",
    "# llm = ZhipuAILLM(temperature=0,api_key=os.environ['ZHIPUAI_API_KEY'])\n",
    "llm = ZhipuAILLM(\n",
    "    model = 'glm-4',\n",
    "    max_tokens = 256,\n",
    "    temperature = 0.8,\n",
    "    api_key=os.environ['ZHIPUAI_API_KEY']\n",
    ")\n",
    "# Split docs\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(data)\n",
    "def generate_response(input_text, openai_api_key,llm = llm):\n",
    "    # llm = ChatOpenAI(temperature=0.7, openai_api_key=openai_api_key)\n",
    "    output = llm.invoke(input_text)\n",
    "    output_parser = StrOutputParser()\n",
    "    output = output_parser.invoke(output)\n",
    "    #st.info(output)\n",
    "    return output\n",
    "\n",
    "def get_vectordb(docs = docs,embedding = embedding):\n",
    "    MILVUS_HOST = 'localhost'\n",
    "    MILVUS_PORT = '19530' # Inference Arguments\n",
    "    # å®šä¹‰ Embeddings\n",
    "    # embedding = ZhipuAIEmbeddings()\n",
    "    \n",
    "    # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„\n",
    "\n",
    "    # persist_directory = './chroma'\n",
    "    # åŠ è½½æ•°æ®åº“\n",
    "    # vectordb = Chroma(\n",
    "    #     persist_directory=persist_directory,  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š\n",
    "    #     embedding_function=embedding\n",
    "    # )\n",
    "    vectordb = Milvus.from_documents(\n",
    "    docs,\n",
    "    embedding,\n",
    "    collection_name='test',\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT},\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "#å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾\n",
    "def get_chat_qa_chain(question:str,openai_api_key:str,llm = llm):\n",
    "    vectordb = get_vectordb()\n",
    "    # llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0,openai_api_key = openai_api_key)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´ã€‚\n",
    "        return_messages=True  # å°†ä»¥æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²\n",
    "    )\n",
    "    retriever=vectordb.as_retriever()\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory\n",
    "    )\n",
    "    result = qa({\"question\": question})\n",
    "    return result['answer']\n",
    "\n",
    "#ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾\n",
    "def get_qa_chain(question:str,openai_api_key:str,llm = llm):\n",
    "    vectordb = get_vectordb()\n",
    "    # llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0,openai_api_key = openai_api_key)\n",
    "    template = \"\"\"ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”\n",
    "        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚\n",
    "        {context}\n",
    "        é—®é¢˜: {question}\n",
    "        \"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result[\"result\"]\n",
    "\n",
    "\n",
    "# Streamlit åº”ç”¨ç¨‹åºç•Œé¢\n",
    "def main():\n",
    "    st.title('ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘')\n",
    "    openai_api_key = st.sidebar.text_input('æš—å·', type='password')\n",
    "\n",
    "    # æ·»åŠ ä¸€ä¸ªé€‰æ‹©æŒ‰é’®æ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹\n",
    "    #selected_method = st.sidebar.selectbox(\"é€‰æ‹©æ¨¡å¼\", [\"qa_chain\", \"chat_qa_chain\", \"None\"])\n",
    "    selected_method = st.radio(\n",
    "        \"ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ\",\n",
    "        [\"None\", \"qa_chain\", \"chat_qa_chain\"],\n",
    "        captions = [\"ä¸ä½¿ç”¨æ£€ç´¢é—®ç­”çš„æ™®é€šæ¨¡å¼\", \"ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼\", \"å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼\"])\n",
    "\n",
    "    # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    messages = st.container(height=300)\n",
    "    if prompt := st.chat_input(\"Say something\"):\n",
    "        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"text\": prompt})\n",
    "\n",
    "        if selected_method == \"None\":\n",
    "            # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”\n",
    "            answer = generate_response(prompt, openai_api_key)\n",
    "        elif selected_method == \"qa_chain\":\n",
    "            answer = get_qa_chain(prompt,openai_api_key)\n",
    "        elif selected_method == \"chat_qa_chain\":\n",
    "            answer = get_chat_qa_chain(prompt,openai_api_key)\n",
    "\n",
    "        # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None\n",
    "        if answer is not None:\n",
    "            # å°†LLMçš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"text\": answer})\n",
    "\n",
    "        # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²\n",
    "        for message in st.session_state.messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                messages.chat_message(\"user\").write(message[\"text\"])\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                messages.chat_message(\"assistant\").write(message[\"text\"])   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¼˜åŒ–ç‚¹\n",
    "1. Chromaæœ¬åœ°å‘é‡åº“æ”¹ä¸º Milvus çº¿ä¸Šå‘é‡åº“\n",
    "2. å…ƒæ•°æ®ä¸€åŒå­˜å‚¨åˆ°Milvusï¼Œæ”¯æŒæ··åˆæ£€ç´¢\n",
    "3. çº¿ä¸Šçš„ embedding æ¨¡å‹æ”¹ä¸ºæœ¬åœ°çš„embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
