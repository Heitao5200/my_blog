# 大模型案例分析

## 个人知识库助手项目

### 项目背景

利用RAG技术结合检索和生成，提高信息处理的准确性和可靠性。

### 整体架构图

![image-20240630234157784](img/Task6//image-20240630234157784.png)



**LLM层**：

- 包含对多种大型语言模型（LLM）的调用封装，如GPT、文心、星火和智谱AI，支持统一入口和方式访问不同模型。

**数据层**：

- 由个人知识库的源数据和Embedding API组成，源数据经过Embedding处理后可供向量数据库使用。

**数据库层**：

- 基于个人知识库源数据搭建的向量数据库 

**应用层**：

- 基于LangChain提供的检索问答链基类进行进一步封装，支持不同模型的切换和基于数据库的检索问答。

**服务层**：

- 包括Gradio和FastAPI两种服务访问方式，分别用于构建Demo和API服务，以便用户与知识库助手进行交云。

### 人情世故大模型系统-天机

## 项目背景

利用大模型进行涉及传统人情世故的任务

### 整体架构

**数据提取**

- 数据清洗、数据处理以及元数据提取等操作。

**向量化（embedding）**

- 将文本、图像、音频和视频等转化为向量矩阵的过程，也就是变成计算机可以理解的格式，embedding模型的好坏会直接影响到后面检索的质量，特别是相关度。

**检索**

- 这是RAG的关键环节，我们可以通过多种检索方式来提升效率和相关度。（如：数据分块、专业领域的嵌入模型微调、查询嵌入的优化等等）

**生成**

- LLM将检索到的信息转化为流畅文本，该文本将成为模型输出的最终成果。