{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本多路召回与重排序\n",
    "### 任务\n",
    "- 任务说明：实现多种文本编码和检索逻辑，并进行重排序\n",
    "- 任务要求：\n",
    "    - 结合文本索引和向量检索结果\n",
    "    - 加载重排序模型，对检索进行重排序\n",
    "- 打卡要求：完成多路召回与重排序，与任务5精度进行对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba, json, pdfplumber\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from rank_bm25 import BM25Okapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = json.load(open(\"./data/questions.json\"))\n",
    "\n",
    "pdf = pdfplumber.open(\"./data/初赛训练数据集.pdf\")\n",
    "pdf_content = []\n",
    "for page_idx in range(len(pdf.pages)):\n",
    "    pdf_content.append({\n",
    "        'page': 'page_' + str(page_idx + 1),\n",
    "        'content': pdf.pages[page_idx].extract_text()\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/code/Xorbits/bge-small-zh-v1___5/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载重排序模型\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/root/code/Xorbits/bge-small-zh-v1___5/')\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained('/root/code/Xorbits/bge-small-zh-v1___5/')\n",
    "rerank_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_content_words = [jieba.lcut(x['content']) for x in pdf_content]\n",
    "bm25 = BM25Okapi(pdf_content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['请问在汽车技术中，1牛米（Nm）的力矩等于多少牛顿（N）乘以米（m）？',\n",
       "  '泊车\\n泊车辅助系统检测范围\\n检测区域 检测范围（米）\\n后部两侧 0.3\\n后部中央 1.5\\n前部两侧 0.3\\n前部中央 0.9\\n泊车辅助系统局限性\\n关于泊车辅助系统局限性，请参见泊车辅助传感器章节。 车辆启动后，您将换挡杆切换至倒挡（R）进行倒车时，后泊车辅助\\n系统自动启用，声音警告信号在车辆离后方物体1.5m时启用。\\n泊车辅助系统\\n警告！\\n车辆低速行驶时，泊车辅助系统通过雷达传感器检测车辆后方是否有\\n障碍物，并通过声音提示驾驶员车辆与障碍物之间的距离。 ■ 作为驾驶员，您应遵守相关法律要求，并对安全停车承担全部责\\n任。\\n■ 泊车辅助系统是一种辅助功能，在使用过程中您仍需注意观察周\\n围环境。\\n225'],\n",
       " ['请问在汽车技术中，1牛米（Nm）的力矩等于多少牛顿（N）乘以米（m）？',\n",
       "  '技术资料\\n说明！ 汽车轴荷（满载状态）\\n□ 请保持挡风玻璃洁净、干燥；请不要在安装汽车电子标识处贴膜 前轴最大轴荷 后轴最大轴荷\\n或粘贴金属材料，确保汽车电子标识的规范安装和数据的有效读 车型 （kg） （kg）\\n取。\\nMR6453DCHEV02 1214 1116\\n车辆参数\\n发动机规格\\n车辆尺寸参数（整备质量） 最大净功率 额定功率 最大扭矩\\n发动机\\n（kW/rpm） （kW/rpm） （Nm/rpm）\\n尺寸 单位（mm）\\n225/（2500-\\nDHE15-ESZ 108/5500 110/5500\\n长度 4549 4000）\\n高度 1689（a）\\n电机规格\\n宽度 1860（b）\\n（a）：测量高度包含多波段天线。 电机类型 峰值功率（kW）\\n（b）：车辆宽度为不包括外后视镜的车宽。\\n永磁同步电机 100\\n汽车重量\\n动力电池规格\\n最大允许总重量\\n车型 整备质量（kg）\\n（kg）\\n电池类型 容量（Ah） 额定电压（V）\\nMR6453DCHEV02 1890 2330\\n锂离子电池 51 347.5\\n349'],\n",
       " ['请问在汽车技术中，1牛米（Nm）的力矩等于多少牛顿（N）乘以米（m）？',\n",
       "  '技术资料\\n公制术语\\n术语 说明\\n术语 说明\\nN 牛\\n% 百分比 Nm 牛米\\nX:1 比值 V 伏特\\n℃ 摄氏温度 W 瓦特\\nAh 安时 kPa 千帕\\nm 米 kW 千瓦\\ncm 厘米\\nmm 毫米\\ng 克\\nkg 千克\\nh 小时\\nmin 分钟\\ns 秒\\nrpm 每分钟转数\\nkm/h 千米每小时\\nL 升\\nmL 毫升\\n354']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.conda/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/root/.conda/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/root/.conda/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/code/Xorbits/bge-small-zh-v1___5/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.564 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "for query_idx in range(len(questions)):\n",
    "    # 首先进行BM25检索\n",
    "    doc_scores = bm25.get_scores(jieba.lcut(questions[query_idx][\"question\"]))\n",
    "    max_score_page_idxs = doc_scores.argsort()[-3:]\n",
    "    \t\n",
    "    # top3进行重排序\n",
    "    pairs = []\n",
    "    for idx in max_score_page_idxs:\n",
    "        pairs.append([questions[query_idx][\"question\"], pdf_content[idx]['content']])\n",
    "\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    with torch.no_grad():\n",
    "        inputs = {key: inputs[key].cuda() for key in inputs.keys()}\n",
    "        scores = rerank_model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "\n",
    "    max_score_page_idx = max_score_page_idxs[scores.cpu().numpy().argmax()]\n",
    "    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx + 1)\n",
    "\n",
    "with open('submit.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
