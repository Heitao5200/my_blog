# 大模型「训练」与「微调」概念详解

## **预训练与微调介绍**

大模型的预训练与微调的目的是：告诉模型**想要什么答案**和**不想要什么答案** 

- **预训练**是为了让模型学习**通用知识**
  - Post-pretraining 是为了让模型更好地适应特定领域，

- 微调是为了优化模型在特定任务上的表现。

  - **SFT** 是微调的一种，侧重于**有监督**的学习，

  - RLHF 是一种结合了**人类反馈的微调**方法，旨在提高模型在复杂任务上的性能。

## **训练**

训练的目标是使模型能够准确地从输入数据中学习到输出结果，从而在未见过的数据上也能做出准确的预测或决策。这个过程通常涉及到以下几个步骤：

- **前向传播：**模型根据当前参数对输入数据进行预测。
- **损失计算：**计算模型预测结果和实际结果之间的差异（损失）。
- **反向传播：**根据损失函数计算对模型参数的梯度（即损失对每个参数的导数）。
- **参数更新：**使用梯度下降或其他优化算法调整模型参数，以减少损失。



## **预训练**(Pre-training)

预训练的目的：让模型在一个广泛的数据集上学习到一些通用的特征或知识，即通用的大模型。

预训练的步骤包括：

- 选择一个大型的、通用的数据集：这个数据集不需要和模型最终的任务完全相关，但它应该足够大和多样化，能够让模型学习到广泛的特征和模式。
- 训练模型：在这个大型数据集上训练模型，使其学习到通用的知识或特征。
- 保存预训练模型：完成预训练后，保存模型的参数。这些参数可以作为后续特定任务训练的起点。
- 预训练模型通常使用自监督学习，这意味着模型试图从输入数据本身预测某些未知的或被遮蔽的部分，例如，预测句子中缺失的单词。
- 预训练的结果是一个通用的模型，它对语言有一个基本的理解，但还没有针对任何特定任务进行优化。
- 预训练的好处是显著减少了模型在特定任务上训练所需的数据量和时间，同时提高了模型在这些任务上的性能。这种方法在自然语言处理（NLP）、计算机视觉等领域尤其流行和有效。

## 后期预训练

Post-pretraining（后期预训练）是一种在模型的初始预训练和最终微调之间进行的训练方法。这种方法通常用于进一步适应模型以处理特定类型的数据或任务。 

### **后期预训练（Post-pretraining）特点**

- Post-pretraining是在通用预训练模型的基础上，对模型进行额外训练的过程，通常是为了使模型更好地适应特定的领域或任务。
- 这个过程使用的数据集通常**比预训练阶段的数据集更专注于某个领域或任务**，但比微调阶段使用的数据集更大、更广泛。
- 训练方法可以是监督学习，也可以是自监督学习，具体取决于可用数据的类型和训练目标。
- Post-pretraining的目标是在不过度专化到某个特定任务的同时，提高模型对特定领域的理解和表现。

- 这个阶段的训练可以包括多种任务，如语言建模、文本分类、实体识别等，这些任务都是为了提升模型在特定领域的表现。
- 训练过程中，模型的参数会根据领域特定数据集进行调整，以便更好地捕捉和理解领域内的语言模式和知识。

**优势与目标**

- Post-pretraining允许模型在保持通用性的同时，增强对特定领域的理解，这有助于模型在后续的微调阶段更快速地适应特定任务。
- 与 SFT 相比，Post-pretraining在微调之前提供了一个中间步骤，有助于模型更平滑地过渡到特定任务上。
- 与 RLHF 相比，Post-pretraining不依赖于复杂的奖励机制或人类反馈，而是通过大量的领域特定数据来提升模型性能。
- 总结来说，Post-pretraining是一个**介于预训练和微调之间的训练阶段**，它使用大量的领域特定数据来进一步调整模型，使其更好地理解特定领域的语言和任务。这个阶段不需要复杂的奖励机制，而是通过传统的监督或自监督学习方法来实现模型性能的提升。

## **微调** (Fine-tuning)

- 在这个阶段，预训练模型（可能经过了Post-pretraining）被进一步训练，以优化它在一个特定任务上的表现。
- 微调通常在一个相对较小的、特定任务的数据集上进行，这个数据集包含了明确的标签，**模型通过监督学习**来进行优化。
- 微调的目的是调整模型的参数，使其能够在特定任务上做出准确的预测。



### **SFT 监督微调**

在SFT(Supervised Fine-Tuning) 阶段，我们使用特定领域的数据或私有化数据对预训练模型进行改良。这一阶段需要指令微调数据，数据集通常由**输入（用户问题）**和**输出（标准答案）**两个字段构成。标准答案通常由专家标注生成。

- SFT是一种简单的微调方法，它使用带有正确答案的数据集来继续训练一个预训练的模型。

- 这种方法依赖于大量的标注数据，即每个输入都有一个预先定义的正确输出。

- 微调的目的是使模型更好地适应特定的任务或领域【垂直领域】，比如特定类型的语言理解或生成任务。

- SFT通常不涉及复杂的策略或奖励函数，只是简单地最小化预测输出和真实输出之间的差异。

### **RLHF 人类反馈强化学习**

RLHF是一种利用人类反馈来训练强化学习模型的方法。在RLHF中，模型通过与人类交互获得反馈，这些反馈作为奖励信号来指导模型的行为。RLHF通常用于训练能够生成更自然、更符合人类偏好的文本或其他输出的模型。这种方法特别适用于需要模型理解和适应人类偏好的场景。

- RLHF (Reinforcement Learning from Human Feedback) 是一种更复杂的训练方法，它结合了监督学习和强化学习。

- 在RLHF中，模型首先通过监督学习进行预训练，然后通过人类提供的反馈来进行强化学习。

- 人类反馈可以是对模型输出的评分，或者是在模型输出之间做出选择的偏好。

- 强化学习部分涉及到定义一个奖励函数，该函数根据人类反馈来调整模型的行为，以优化长期的奖励。

- RLHF的目标是训练出一个在没有明确标签的复杂任务中表现良好的模型，这些任务可能需要更细致的判断和调整。

### **模型对齐**

对齐阶段目的是进一步优化模型，使其更符合实际应用需求。在这个阶段，我们收集用户反馈数据（如点赞或点踩），并基于这些数据进行模型的进一步训练。

对齐阶段的数据格式与SFT阶段不同：通常包含对同一问题的**接受（accept）和拒绝（reject）**两种答案。

在SFT阶段，模型被训练以识别“想要的答案”，但未明确告知“不想要的答案”。为解决这一问题，我们**通过收集用户反馈和日志数据**，在对齐阶段告诉模型哪些答案是不可接受的。

 经过SFT和对齐阶段的训练，我们可以得到一个优化后的模型，这个模型可以部署上线。在对齐过程中，我们可以使用一些常见的方法，如PPO（Proximal Policy Optimization）和DPO（Distributional Proximal Optimization）。DPO由于训练过程相对简单，已成为对齐阶段的主流算法。 

总的来说，SFT更侧重于直接从标注数据中学习，而RLHF则试图通过人类的反馈来引导模型学习更复杂和更细粒度的行为。RLHF通常被认为是一种更接近人类学习方式的方法，因为它不仅仅依赖于标签数据，还依赖于人类对模型输出的评价和偏好。



### **RLHF与模型对齐区别**

总的来说，模型对齐阶段可以视为一个更广泛的概念，而RLHF是一种特定的实现方式，特别是在强化学习领域。两者在实践中可能会有交集，但它们侧重点和应用方式有所不同。

**1、联系：**两者都涉及到根据反馈来调整模型的行为，以提高模型的性能和适应性。

**2、区别：**

- 技术实现：对齐阶段可能不仅限于强化学习，还可以包括监督学习或其他类型的学习；而RLHF明确使用了强化学习框架。
- 反馈来源：对齐阶段的反馈可以来自用户的实际使用情况，而RLHF的反馈通常来自与模型交互的人类评估者。
- 目标：对齐阶段的目标是使模型的输出与用户期望对齐，而RLHF的目标是通过人类反馈来优化模型的决策过程。







### **Lora & Q-Lora**





#### **Lora（Low-Rank Adaptation）**

LoRA 是一种微调技术，它通过在模型的权重矩阵中引入**低秩结构**来进行微调。具体来说，它将原始的权重矩阵分解为两个较小的矩阵的乘积，这两个较小的矩阵在微调过程中被更新，而原始的权重矩阵保持不变。这样，只需要对模型的一小部分参数进行更新，从而减少了计算量和存储需求。

**低秩适应**是一种有效的**模型压缩和加速技术**，特别适用于大型模型，因为它可以**显著减少模型的参数数量**，同时保持性能。



#### **Q-Lora（**Quantized Lora**）**

Q-LoRA 是 LoRA 的一种变体，它结合了量化和低秩适应的概念。在 Q-LoRA 中，除了使用低秩矩阵来减少参数数量外，还会对这些矩阵进行量化，即将它们从浮点数转换为整数形式，如Int8或Int16。



量化可以进一步减少模型的大小，加快模型的推理速度，并且可以在某些硬件上实现更高效的计算。Q-LoRA通过结合量化和低秩适应，旨在实现更高效的模型微调，特别是在资源受限的环境中。



这些技术对于在保持模型性能的同时减少模型的计算和存储需求非常有用，特别是在部署大型模型到边缘设备或移动设备时。通过 LoRA 或 Q-LoRA，可以以较低的成本对大型模型进行微调，使其适应特定的应用场景。

LoRA 与 Q-LoRA **只更新 adapter层 的参数，不会更新原有语言模型的参数。所以微调结束后还需要做模型合并后才能使用微调后的模型。**

 

## **什么时候需要结合微调进一步增强 RAG 能力？**



| 因素           | 原因                                                         |
| -------------- | ------------------------------------------------------------ |
| 领域特定性     | 如果客服需要处理的问题非常专业或者特定于某个行业，那么单纯的RAG(Retrieval-Augmented Generation)加上通用大模型可能不足以提供精确的答案。在这种情况下，微调模型以包含特定领域的数据和术语，可以显著提高客服的准确性和效率。 |
| 上下文理解     | 通用大模型在处理长上下文和复杂对话时可能表现不佳。如果客服场景需要深入理解客户的历史对话和特定需求，微调模型以更好地理解上下文和用户意图可能会更有效。 |
| 法律和合规性   | 在某些行业，如金融或医疗，客服需要严格遵守特定的法规和标准。微调模型以确保所有回答都符合这些要求是至关重要的。 |
| 个性化服务     | 如果企业希望提供高度个性化的客户服务，例如根据用户的购买历史、偏好和行为来定制回答，那么微调模型以包含这些个性化数据是必要的。 |
| 文化和语言差异 | 不同地区和文化背景的用户可能有不同的沟通方式和表达习惯。如果客服需要服务于多语言或多文化的用户，微调模型以适应这些差异可以提供更好的用户体验。 |
| 性能和资源     | 微调模型通常需要额外的计算资源和时间。如果企业有足够的资源来支持微调，并且期望通过微调获得显著的性能和准确度的提升，那么这是一个值得考虑的选项。 |
| 持续学习和适应 | 如果客服系统需要不断适应新的数据和用户行为，那么微调可以是一个持续的过程，而不是一次性的事件。 |



## 参考资料

https://mp.weixin.qq.com/s/YCRfJKx60wel5BEFgt74iw

https://mp.weixin.qq.com/s/b5LzVJi4Jlg-OIy1RexTOw











XMIND



- 大模型的预训练与微调介绍
  - 预训练概念
  - 后期预训练概念
  - 微调
    - SFT
    - RLHF
    - Lora & Q-Lora
    - 模型对齐
  - 需要微调的场景
- 模型微调的步骤

  - 微调环境准备
  - 数据准备
  - 微调启动
  - 微调后参数转换/合并
- 模型微调的案例实战

  